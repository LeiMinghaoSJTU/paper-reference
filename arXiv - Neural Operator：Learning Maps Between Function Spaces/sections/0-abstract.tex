神经网络的经典发展主要聚焦于学习有限维欧几里得空间或有限集合之间的映射。我们提出将神经网络推广至算子学习领域，这类算子被称为 “神经算子”（neural operators），可在无限维函数空间之间实现映射。
我们将神经算子表述为由线性积分算子与非线性激活函数构成的组合。针对所提神经算子，我们证明了其满足通用逼近定理 —— 这表明它能够逼近任意给定的非线性连续算子。此外，所提神经算子还具有离散化不变性：即在底层函数空间的不同离散化形式下，它们可共享相同的模型参数。
同时，我们提出了四类高效参数化方法，即图神经算子（graph neural operators）、多极图神经算子（multi-pole graph neural operators）、低秩神经算子（low-rank neural operators）以及傅里叶神经算子（Fourier neural operators）。
神经算子的一个重要应用场景是为偏微分方程（PDE）求解算子学习替代映射。我们以伯格斯方程（Burgers）、达西地下水流方程（Darcy subsurface flow）和纳维 - 斯托克斯方程（Navier-Stokes equations）等标准 PDE 为研究对象，结果表明：所提神经算子相较于现有基于机器学习的方法具有更优性能，同时其速度比传统 PDE 求解器快数个数量级。