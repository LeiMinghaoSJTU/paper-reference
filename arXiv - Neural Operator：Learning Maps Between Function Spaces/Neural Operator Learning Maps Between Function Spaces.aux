\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand*\HyPL@Entry[1]{}
\bibstyle{plainnat}
\HyPL@Entry{0<</S/D>>}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{2}{section.1}\protected@file@percent }
\newlabel{sec:I}{{1}{2}{}{section.1}{}}
\citation{scarselli2008graph}
\citation{vaswani2017attention}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Our Approach}{3}{subsection.1.1}\protected@file@percent }
\newlabel{ssec:OC}{{1.1}{3}{}{subsection.1.1}{}}
\citation{hornik1989multilayer}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Discretization Invariance}}{4}{figure.caption.1}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:discretization-invariance}{{1}{4}{Discretization Invariance}{figure.caption.1}{}}
\citation{chandler2013invariant}
\citation{gurtin1982introduction}
\citation{johnson2012numerical}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Neural operator architecture schematic}}{6}{figure.caption.2}\protected@file@percent }
\newlabel{fig:NO_architecture}{{2}{6}{Neural operator architecture schematic}{figure.caption.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Background and Context}{6}{subsection.1.2}\protected@file@percent }
\newlabel{ssec:LR}{{1.2}{6}{}{subsection.1.2}{}}
\citation{belongie2002spectral}
\citation{von2008consistency,trillos2018variational,trillos2020error}
\citation{boyd2001chebyshev,trefethen2000spectral}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Comparison of deep learning models. The first row indicates whether the model is discretization invariant. The second and third rows indicate whether the output and input are a functions. The fourth row indicates whether the model class is a universal approximator of operators. Neural Operators are discretization invariant deep learning methods that output functions and can approximate any operator.}}{7}{table.caption.3}\protected@file@percent }
\newlabel{table:deeplearning_comparison}{{1}{7}{Comparison of deep learning models. The first row indicates whether the model is discretization invariant. The second and third rows indicate whether the output and input are a functions. The fourth row indicates whether the model class is a universal approximator of operators. Neural Operators are discretization invariant deep learning methods that output functions and can approximate any operator}{table.caption.3}{}}
\citation{raissi2019physics}
\citation{lu2019deeponet}
\citation{de2022cost}
\citation{vaswani2017attention}
\citation{dosovitskiy2020image}
\citation{lu2019deeponet}
\@writefile{toc}{\contentsline {section}{\numberline {2}Learning Operators}{9}{section.2}\protected@file@percent }
\newlabel{sec:setting}{{2}{9}{}{section.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Generic Parametric PDEs}{9}{subsection.2.1}\protected@file@percent }
\newlabel{sec:genericPDE}{{2.1}{9}{}{subsection.2.1}{}}
\newlabel{eq:generalpde0}{{1}{9}{}{equation.1}{}}
\citation{Vapnik1998}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Problem Setting}{10}{subsection.2.2}\protected@file@percent }
\newlabel{sec:PS}{{2.2}{10}{}{subsection.2.2}{}}
\newlabel{eq:approxmap2}{{2}{10}{}{equation.2}{}}
\newlabel{eq:bochner_error}{{3}{10}{}{equation.3}{}}
\newlabel{eq:empirical_risk}{{4}{10}{}{equation.4}{}}
\citation{hornik1989multilayer}
\newlabel{eq:unform_risk}{{5}{11}{}{equation.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Discretization}{11}{subsection.2.3}\protected@file@percent }
\newlabel{sec:discretization}{{2.3}{11}{}{subsection.2.3}{}}
\newlabel{def:discretization_invariance}{{4}{12}{}{theorem.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Neural Operators}{13}{section.3}\protected@file@percent }
\newlabel{sec:neuraloperators}{{3}{13}{}{section.3}{}}
\citation{dudley2011concrete}
\citation{he2016deep}
\newlabel{eq:F}{{6}{14}{}{equation.6}{}}
\newlabel{eq:kernelop1}{{7}{15}{}{equation.7}{}}
\newlabel{eq:kernelop2}{{8}{15}{}{equation.8}{}}
\citation{vaswani2017attention}
\newlabel{eq:kernelop3}{{9}{16}{}{equation.9}{}}
\newlabel{eq:onelayer}{{10}{16}{}{equation.10}{}}
\newlabel{eq:singlehiddenlayer}{{11}{16}{}{equation.11}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Parameterization and Computation}{18}{section.4}\protected@file@percent }
\newlabel{sec:four_schemes}{{4}{18}{}{section.4}{}}
\newlabel{eq:onelayercompute}{{12}{18}{}{equation.12}{}}
\newlabel{eq:onelayerlinear}{{13}{19}{}{equation.13}{}}
\newlabel{eq:onelayerlinear_MC}{{14}{19}{}{equation.14}{}}
\citation{li2020neural}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Graph Neural Operator (GNO)}{20}{subsection.4.1}\protected@file@percent }
\newlabel{sec:graphneuraloperator}{{4.1}{20}{}{subsection.4.1}{}}
\newlabel{eq:nystrom_matrix}{{15}{20}{}{equation.15}{}}
\citation{gilmer2017neural}
\newlabel{eq:onelayertruncation}{{16}{21}{}{equation.16}{}}
\citation{gilmer2017neural}
\citation{pfaff2020learning}
\newlabel{eq:onelayercnn}{{17}{23}{}{equation.17}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Low-rank Neural Operator (LNO)}{24}{subsection.4.2}\protected@file@percent }
\newlabel{sec:lowrank}{{4.2}{24}{}{subsection.4.2}{}}
\newlabel{eq:finiteranksvd}{{18}{24}{}{equation.18}{}}
\citation{lu2019deeponet}
\citation{e2011principles}
\citation{li2020multipole}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Multipole Graph Neural Operator (MGNO)}{25}{subsection.4.3}\protected@file@percent }
\newlabel{sec:multipole}{{4.3}{25}{}{subsection.4.3}{}}
\newlabel{eq:decomposition_matrix}{{19}{25}{}{equation.19}{}}
\citation{greengard1997new}
\citation{quinonero2005aunifying}
\citation{kondor2014multiresolution}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Hierarchical matrix decomposition}}{26}{figure.caption.4}\protected@file@percent }
\newlabel{fig:hmatrix}{{3}{26}{Hierarchical matrix decomposition}{figure.caption.4}{}}
\newlabel{eq:hierarchy_decomposition}{{20}{27}{}{equation.20}{}}
\newlabel{eq:all1}{{21}{27}{}{equation.21}{}}
\newlabel{eq:all2}{{22}{27}{}{equation.22}{}}
\newlabel{eq:all3}{{23}{27}{}{equation.23}{}}
\citation{greengard1997new}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces V-cycle}}{28}{figure.caption.5}\protected@file@percent }
\newlabel{fig:vcycle}{{4}{28}{V-cycle}{figure.caption.5}{}}
\newlabel{fig:multigraph}{{4}{28}{V-cycle}{figure.caption.5}{}}
\newlabel{eq:up}{{24}{28}{}{equation.24}{}}
\newlabel{eq:down}{{25}{28}{}{equation.25}{}}
\citation{nelsen2020random}
\citation{li2020fourier}
\citation{kovachki2021universal}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}Fourier Neural Operator (FNO)}{29}{subsection.4.4}\protected@file@percent }
\newlabel{sec:fourier}{{4.4}{29}{}{subsection.4.4}{}}
\newlabel{eq:fourierlayer}{{26}{30}{}{equation.26}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces  {\bf  top:} The architecture of the neural operators; \textbf  {bottom:} Fourier layer.}}{31}{figure.caption.6}\protected@file@percent }
\newlabel{fig:arch}{{5}{31}{{\bf top:} The architecture of the neural operators; \textbf {bottom:} Fourier layer}{figure.caption.6}{}}
\newlabel{eq:fft_mult}{{27}{32}{}{equation.27}{}}
\citation{bruno2007accurate}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5}Summary}{34}{subsection.4.5}\protected@file@percent }
\citation{lu2019deeponet}
\@writefile{toc}{\contentsline {section}{\numberline {5}Neural Operators and Other Deep Learning Models}{35}{section.5}\protected@file@percent }
\newlabel{sec:framework}{{5}{35}{}{section.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}DeepONets}{35}{subsection.5.1}\protected@file@percent }
\newlabel{sec:deeponets}{{5.1}{35}{}{subsection.5.1}{}}
\newlabel{prop:deeponet}{{5}{36}{}{theorem.5}{}}
\newlabel{eq:likedeeponet}{{32}{36}{}{equation.32}{}}
\citation{lanthaler2021error}
\citation{de2022cost}
\citation{devore1998nonlinear}
\citation{pinkus1985nwidths}
\citation{cohendevore}
\newlabel{eq:likedeeponet-operator}{{33}{37}{}{equation.33}{}}
\citation{devore1998nonlinear}
\citation{bonito2020nonlinear,cohen2020optimal}
\citation{Kovachki}
\citation{lu2019deeponet}
\citation{lu2021comprehensive}
\citation{Kovachki}
\citation{box1976science}
\citation{lu2021comprehensive}
\citation{lu2019deeponet}
\citation{lu2021comprehensive}
\citation{lu2021comprehensive}
\citation{bruno2007accurate}
\citation{vaswani2017attention}
\citation{devlin2018bert,brown2020language}
\citation{dosovitskiy2020image}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Transformers as a Special Case of Neural Operators}{39}{subsection.5.2}\protected@file@percent }
\newlabel{sec:transformers}{{5.2}{39}{}{subsection.5.2}{}}
\citation{vaswani2017attention}
\newlabel{prop:attention}{{6}{40}{}{theorem.6}{}}
\newlabel{eq:liketransformer}{{34}{40}{}{equation.34}{}}
\newlabel{eq:continoustransformer}{{35}{40}{}{equation.35}{}}
\citation{ba2016layer}
\newlabel{eq:discretetransformer}{{36}{41}{}{equation.36}{}}
\citation{zhu2021long}
\citation{guibas2021adaptive}
\citation{choromanski2020rethinking,dosovitskiy2020image}
\@writefile{toc}{\contentsline {section}{\numberline {6}Test Problems}{42}{section.6}\protected@file@percent }
\newlabel{sec:problems}{{6}{42}{}{section.6}{}}
\newlabel{eq:generalpde}{{37}{42}{}{equation.37}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}Poisson Equation}{43}{subsection.6.1}\protected@file@percent }
\newlabel{ssec:poisson}{{6.1}{43}{}{subsection.6.1}{}}
\newlabel{eq:poisson}{{38}{43}{}{equation.38}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2}Darcy Flow}{44}{subsection.6.2}\protected@file@percent }
\newlabel{ssec:darcy}{{6.2}{44}{}{subsection.6.2}{}}
\newlabel{eq:darcy}{{39}{44}{}{equation.39}{}}
\newlabel{eq:darcysolutionop}{{40}{45}{}{equation.40}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3}Burgers' Equation}{45}{subsection.6.3}\protected@file@percent }
\newlabel{ssec:burgers}{{6.3}{45}{}{subsection.6.3}{}}
\newlabel{eq:burgers}{{41}{45}{}{equation.41}{}}
\newlabel{eq:burgerssolutionop}{{42}{46}{}{equation.42}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.4}Navier-Stokes Equation}{46}{subsection.6.4}\protected@file@percent }
\newlabel{ssec:nse}{{6.4}{46}{}{subsection.6.4}{}}
\newlabel{eq:navierstokes-velocity}{{43}{46}{}{equation.43}{}}
\newlabel{eq:navierstokes}{{44}{46}{}{equation.44}{}}
\citation{chandler2013invariant}
\citation{chandler2013invariant}
\newlabel{eq:nssolutionop_add}{{45}{47}{}{equation.45}{}}
\newlabel{eq:nssolutionop2}{{46}{47}{}{equation.46}{}}
\citation{cotter2009bayesian,stuart_2010}
\citation{Cotter_2013}
\citation{Cotter_2013}
\citation{kraichnan67inertial}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.4.1}Bayesian Inverse Problem}{48}{subsubsection.6.4.1}\protected@file@percent }
\newlabel{sec:problem:bayesian}{{6.4.1}{48}{}{subsubsection.6.4.1}{}}
\newlabel{eq:inverseproblem}{{47}{48}{}{equation.47}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.4.2}Spectra}{49}{subsubsection.6.4.2}\protected@file@percent }
\newlabel{app:sepctral}{{6.4.2}{49}{}{subsubsection.6.4.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Spectral Decay of Navier-Stokes.}}{49}{figure.caption.7}\protected@file@percent }
\newlabel{fig:spectral1}{{6}{49}{Spectral Decay of Navier-Stokes}{figure.caption.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.5}Choice of Loss Criteria}{49}{subsection.6.5}\protected@file@percent }
\citation{de2022cost}
\@writefile{toc}{\contentsline {section}{\numberline {7}Numerical Results}{50}{section.7}\protected@file@percent }
\newlabel{sec:numerics}{{7}{50}{}{section.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1}Poisson Equation}{51}{subsection.7.1}\protected@file@percent }
\newlabel{ssec:result_green}{{7.1}{51}{}{subsection.7.1}{}}
\citation{Zabaras}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Kernel for one-dimensional Green's function, with the Nystrom approximation method}}{52}{figure.caption.8}\protected@file@percent }
\newlabel{fig:kernel1d_nystrom}{{7}{52}{Kernel for one-dimensional Green's function, with the Nystrom approximation method}{figure.caption.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2}Darcy and Burgers Equations}{52}{subsection.7.2}\protected@file@percent }
\newlabel{ssec:darcyburgers}{{7.2}{52}{}{subsection.7.2}{}}
\citation{Kovachki}
\citation{DeVoreReducedBasis}
\citation{lu2019deeponet}
\citation{lanthaler2021error}
\citation{lu2019deeponet}
\citation{Kovachki}
\citation{lu2021comprehensive}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.2.1}Darcy Flow}{53}{subsubsection.7.2.1}\protected@file@percent }
\newlabel{ssec:DF}{{7.2.1}{53}{}{subsubsection.7.2.1}{}}
\citation{lu2021comprehensive}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Benchmark on Burger's equation and Darcy Flow {}}}{54}{figure.caption.9}\protected@file@percent }
\newlabel{fig:error}{{8}{54}{Benchmark on Burger's equation and Darcy Flow \done {It is confusing to have the two left hand panels with the right-most panel; the left most panels who resolution invariance very clear;y, the NSE panel is somewhat different. I suggest separate figures.}}{figure.caption.9}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.2.2}Burgers' Equation}{54}{subsubsection.7.2.2}\protected@file@percent }
\newlabel{ssec:BE}{{7.2.2}{54}{}{subsubsection.7.2.2}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Relative error on 2-d Darcy Flow for different resolutions $s$.}}{55}{table.caption.10}\protected@file@percent }
\newlabel{table:darcy}{{2}{55}{Relative error on 2-d Darcy Flow for different resolutions $s$}{table.caption.10}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces  Relative errors on 1-d Burgers' equation for different resolutions $s$.}}{55}{table.caption.11}\protected@file@percent }
\newlabel{table:burgers}{{3}{55}{Relative errors on 1-d Burgers' equation for different resolutions $s$}{table.caption.11}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.2.3}Zero-shot super-resolution.}{55}{subsubsection.7.2.3}\protected@file@percent }
\newlabel{sec:superresolution1}{{7.2.3}{55}{}{subsubsection.7.2.3}{}}
\citation{he2016deep}
\citation{ronneberger2015u}
\citation{wang2020towards}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Darcy, trained on $16 \times 16$, tested on $241 \times 241$}}{56}{figure.caption.12}\protected@file@percent }
\newlabel{fig:super1}{{9}{56}{Darcy, trained on $16 \times 16$, tested on $241 \times 241$}{figure.caption.12}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.3}Navier-Stokes Equation}{56}{subsection.7.3}\protected@file@percent }
\newlabel{ssec:result_nse}{{7.3}{56}{}{subsection.7.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Benchmark on the Navier-Stokes}}{57}{figure.caption.13}\protected@file@percent }
\newlabel{fig:ns-error}{{10}{57}{Benchmark on the Navier-Stokes}{figure.caption.13}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces Benchmarks on Navier Stokes (fixing resolution $64 \times 64$ for both training and testing).}}{57}{table.caption.14}\protected@file@percent }
\newlabel{table:ns}{{4}{57}{Benchmarks on Navier Stokes (fixing resolution $64 \times 64$ for both training and testing)}{table.caption.14}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces  Resolution study on Navier-stokes equation ($\nu =10^{-3}$, $N=200$, $T=20$.)}}{58}{table.caption.15}\protected@file@percent }
\newlabel{table:nse}{{5}{58}{Resolution study on Navier-stokes equation ($\nu =10^{-3}$, $N=200$, $T=20$.)}{table.caption.15}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.3.1}Zero-shot super-resolution.}{58}{subsubsection.7.3.1}\protected@file@percent }
\newlabel{sec:superresolution}{{7.3.1}{58}{}{subsubsection.7.3.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces  Zero-shot super-resolution}}{59}{figure.caption.16}\protected@file@percent }
\newlabel{fig:super2}{{11}{59}{Zero-shot super-resolution}{figure.caption.16}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.3.2}Spectral analysis}{59}{subsubsection.7.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.3.3}Non-periodic boundary condition.}{59}{subsubsection.7.3.3}\protected@file@percent }
\citation{Cotter_2013}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces The spectral decay of the predictions of different methods}}{60}{figure.caption.17}\protected@file@percent }
\newlabel{fig:spectra}{{12}{60}{The spectral decay of the predictions of different methods}{figure.caption.17}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces Spectral Decay in term of $k_{max}$ }}{60}{figure.caption.18}\protected@file@percent }
\newlabel{fig:spectral2}{{13}{60}{Spectral Decay in term of $k_{max}$}{figure.caption.18}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.3.4}Bayesian Inverse Problem}{60}{subsubsection.7.3.4}\protected@file@percent }
\newlabel{sec:bayesian}{{7.3.4}{60}{}{subsubsection.7.3.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.4}Discussion and Comparison of the Four methods}{61}{subsection.7.4}\protected@file@percent }
\newlabel{ssec:comparsion}{{7.4}{61}{}{subsection.7.4}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.4.1}Ingenuity}{61}{subsubsection.7.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.4.2}Expressiveness}{61}{subsubsection.7.4.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces Results of the Bayesian inverse problem for the Navier-Stokes equation. }}{62}{figure.caption.19}\protected@file@percent }
\newlabel{fig:baysian}{{14}{62}{Results of the Bayesian inverse problem for the Navier-Stokes equation}{figure.caption.19}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6}{\ignorespaces Ingenuity.}}{62}{table.caption.20}\protected@file@percent }
\newlabel{table:ingenuity}{{6}{62}{Ingenuity}{table.caption.20}{}}
\citation{lu2021comprehensive}
\citation{lu2021comprehensive}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.4.3}Complexity}{63}{subsubsection.7.4.3}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {7}{\ignorespaces Complexity (roundup to second on a single Nvidia V100 GPU)}}{63}{table.caption.21}\protected@file@percent }
\newlabel{table:complexity}{{7}{63}{Complexity (roundup to second on a single Nvidia V100 GPU)}{table.caption.21}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.4.4}Refinability}{63}{subsubsection.7.4.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.4.5}Robustness}{63}{subsubsection.7.4.5}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {8}{\ignorespaces Refinability.}}{64}{table.caption.22}\protected@file@percent }
\newlabel{table:refinability}{{8}{64}{Refinability}{table.caption.22}{}}
\@writefile{lot}{\contentsline {table}{\numberline {9}{\ignorespaces Robustness.}}{64}{table.caption.23}\protected@file@percent }
\newlabel{table:robustness}{{9}{64}{Robustness}{table.caption.23}{}}
\citation{pfaff2020learning}
\citation{wen2021u}
\citation{chen1995universal}
\citation{Kovachki}
\citation{chen1995universal}
\citation{lanthaler2021error}
\citation{lu2019deeponet,lu2021learning}
\citation{chen1995universal}
\citation{lanthaler2021error}
\citation{kovachki2021universal}
\@writefile{toc}{\contentsline {section}{\numberline {8}Approximation Theory}{65}{section.8}\protected@file@percent }
\newlabel{sec:approximation}{{8}{65}{}{section.8}{}}
\citation{Kovachki,kovachki2021universal}
\citation{chen1995universal,lanthaler2021error}
\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces Robustness on Advection and Burgers equations}}{66}{figure.caption.24}\protected@file@percent }
\newlabel{fig:robustness}{{15}{66}{Robustness on Advection and Burgers equations}{figure.caption.24}{}}
\citation{grothendieck1955produits}
\citation{pinkus1999approximation}
\citation{lanthaler2021error,Kovachki}
\citation{lanthaler2021error}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.1}Neural Operators}{68}{subsection.8.1}\protected@file@percent }
\newlabel{sec:approximation_nos}{{8.1}{68}{}{subsection.8.1}{}}
\citation{lanthaler2021error}
\newlabel{def:bounded_activations}{{7}{69}{}{theorem.7}{}}
\citation{dudley2010concrete}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.2}Discretization Invariance}{70}{subsection.8.2}\protected@file@percent }
\newlabel{sec:discritizational_invariance}{{8.2}{70}{}{subsection.8.2}{}}
\newlabel{thm:discretizational_invariance}{{8}{70}{}{theorem.8}{}}
\citation{chen1995universal}
\citation{Kovachki}
\@writefile{lof}{\contentsline {figure}{\numberline {16}{\ignorespaces A schematic overview of the maps used to approximate \(\mathcal  {G}^\dagger \).}}{71}{figure.caption.25}\protected@file@percent }
\newlabel{fig:approach}{{16}{71}{A schematic overview of the maps used to approximate \(\G ^\dagger \)}{figure.caption.25}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.3}Approximation Theorems}{71}{subsection.8.3}\protected@file@percent }
\newlabel{sec:approximation_main}{{8.3}{71}{}{subsection.8.3}{}}
\newlabel{assump:input}{{9}{71}{}{theorem.9}{}}
\newlabel{assump:output}{{10}{71}{}{theorem.10}{}}
\newlabel{thm:main_compact}{{11}{72}{}{theorem.11}{}}
\citation{lanthaler2021error}
\citation{Kovachki}
\newlabel{thm:cm_compact}{{12}{73}{}{theorem.12}{}}
\newlabel{thm:measurable_approx}{{13}{73}{}{theorem.13}{}}
\citation{guo2016convolutional,Zabaras,Adler2017,bhatnagar2019prediction,kutyniok2022atheoretical}
\citation{khoo2017solving}
\citation{ummenhofer2020lagrangian}
\citation{jiang2020meshfreeflownet}
\citation{lu2019deeponet,lu2021learning}
\citation{chen1995universal}
\citation{chen1995universal}
\citation{lanthaler2021error}
\newlabel{thm:cm_measurable_approx}{{14}{74}{}{theorem.14}{}}
\@writefile{toc}{\contentsline {section}{\numberline {9}Literature Review}{74}{section.9}\protected@file@percent }
\newlabel{ssec:related-work}{{9}{74}{}{section.9}{}}
\citation{Weinan,raissi2019physics,sirignano2018dgm,bar2019unsupervised,smith2020eikonet,pan2020physics,beck2021solving}
\citation{pathak2020using,um2020solver,greenfeld2019learning}
\citation{DeVoreReducedBasis}
\citation{cohendevore}
\citation{Kovachki,nelsen2020random,opschoor2020deep,schwab2019deep,o2020derivative,lu2019deeponet,fresca2022poddlrom}
\citation{guo2016convolutional,Zabaras,Adler2017,bhatnagar2019prediction}
\citation{Weinan,raissi2019physics,herrmann2020deep,bar2019unsupervised}
\citation{haber2017stable,weinan2017proposal}
\citation{Williams,Neal,BengioLeRoux,GlobersonLivni,Guss}
\citation{Neal,MathewsGP,Garriga-AlonsoGP}
\citation{damianou2013deep,aretha}
\citation{nystrom1930praktische}
\citation{kipf2016semi,hamilton2017inductive,gilmer2017neural,velivckovic2017graph,murphy2018janossy}
\citation{chen2019graph}
\citation{battaglia2018relational}
\citation{pmlr-v97-alet19a}
\citation{kulis2006learning,bach2013sharp,lan2017low,gardner2018product}
\citation{khoo2019switchnet}
\citation{lu2019deeponet}
\citation{greengard1997new}
\citation{borm2003hierarchical}
\citation{kondor2014multiresolution,borm2003hierarchical}
\citation{fan2019multiscale,fan2019multiscale2,he2019mgnet}
\citation{hornik1989multilayer}
\citation{rahimi2008uniform}
\citation{mathieu2013fast}
\citation{bengio2007scaling,mingo2004Fourier,sitzmann2020implicit}
\citation{fan2019bcr,fan2019multiscale,kashinath2020enforcing}
\citation{kovachki2021universal}
\citation{lanthaler2021error,kovachki2021universal}
\@writefile{toc}{\contentsline {section}{\numberline {10}Conclusions}{79}{section.10}\protected@file@percent }
\newlabel{sec:conclusion}{{10}{79}{}{section.10}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.1}Future Directions}{80}{subsection.10.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {10.1.1}New Applications}{80}{subsubsection.10.1.1}\protected@file@percent }
\citation{pathak2020using,um2020solverintheloop}
\citation{wang2021learning,li2021physics}
\citation{lu2019deeponet}
\citation{devore1998nonlinear}
\citation{poole2016exponential}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {10.1.2}New Methodologies}{81}{subsubsection.10.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {10.1.3}Theory}{81}{subsubsection.10.1.3}\protected@file@percent }
\bibdata{ref}
\bibcite{aaronson1997introduction}{{1}{1997}{{Aaronson}}{{}}}
\bibcite{adams2003sobolev}{{2}{2003}{{Adams and Fournier}}{{}}}
\bibcite{Adler2017}{{3}{2017}{{Adler and Oktem}}{{}}}
\bibcite{albiac2006topics}{{4}{2006}{{Albiac and Kalton}}{{}}}
\bibcite{pmlr-v97-alet19a}{{5}{2019}{{Alet et~al.}}{{Alet, Jeewajee, Villalonga, Rodriguez, Lozano-Perez, and Kaelbling}}}
\bibcite{ba2016layer}{{6}{2016}{{Ba et~al.}}{{Ba, Kiros, and Hinton}}}
\bibcite{bach2013sharp}{{7}{2013}{{Bach}}{{}}}
\bibcite{bar2019unsupervised}{{8}{2019}{{Bar and Sochen}}{{}}}
\bibcite{battaglia2018relational}{{9}{2018}{{Battaglia et~al.}}{{Battaglia, Hamrick, Bapst, Sanchez-Gonzalez, Zambaldi, Malinowski, Tacchetti, Raposo, Santoro, Faulkner, et~al.}}}
\bibcite{beck2021solving}{{10}{2021}{{Beck et~al.}}{{Beck, Becker, Grohs, Jaafari, and Jentzen}}}
\bibcite{belongie2002spectral}{{11}{2002}{{Belongie et~al.}}{{Belongie, Fowlkes, Chung, and Malik}}}
\bibcite{bengio2007scaling}{{12}{2007}{{Bengio et~al.}}{{Bengio, LeCun, et~al.}}}
\bibcite{bhatnagar2019prediction}{{13}{2019}{{Bhatnagar et~al.}}{{Bhatnagar, Afshar, Pan, Duraisamy, and Kaushik}}}
\bibcite{Kovachki}{{14}{2020}{{Bhattacharya et~al.}}{{Bhattacharya, Hosseini, Kovachki, and Stuart}}}
\bibcite{bogachev2007measure}{{15}{2007}{{Bogachev}}{{}}}
\bibcite{bonito2020nonlinear}{{16}{2020}{{Bonito et~al.}}{{Bonito, Cohen, DeVore, Guignard, Jantsch, and Petrova}}}
\bibcite{borm2003hierarchical}{{17}{2003}{{B{\"o}rm et~al.}}{{B{\"o}rm, Grasedyck, and Hackbusch}}}
\bibcite{box1976science}{{18}{1976}{{Box}}{{}}}
\bibcite{boyd2001chebyshev}{{19}{2001}{{Boyd}}{{}}}
\bibcite{brown2020language}{{20}{2020}{{Brown et~al.}}{{Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal, Neelakantan, Shyam, Sastry, Askell, et~al.}}}
\bibcite{brudnyi2012methods}{{21}{2012}{{Brudnyi and Brudnyi}}{{}}}
\bibcite{bruno2007accurate}{{22}{2007}{{Bruno et~al.}}{{Bruno, Han, and Pohlman}}}
\bibcite{chandler2013invariant}{{23}{2013}{{Chandler and Kerswell}}{{}}}
\bibcite{chen2019graph}{{24}{2019}{{Chen et~al.}}{{Chen, Ye, Zuo, Zheng, and Ong}}}
\bibcite{chen1995universal}{{25}{1995}{{Chen and Chen}}{{}}}
\bibcite{choromanski2020rethinking}{{26}{2020}{{Choromanski et~al.}}{{Choromanski, Likhosherstov, Dohan, Song, Gane, Sarlos, Hawkins, Davis, Mohiuddin, Kaiser, et~al.}}}
\bibcite{ciesielski1972construction}{{27}{1972}{{Ciesielski and Domsta}}{{}}}
\bibcite{cohendevore}{{28}{2015}{{Cohen and DeVore}}{{}}}
\bibcite{cohen2020optimal}{{29}{2020}{{Cohen et~al.}}{{Cohen, Devore, Petrova, and Wojtaszczyk}}}
\bibcite{conway2007acourse}{{30}{2007}{{Conway}}{{}}}
\bibcite{Cotter_2013}{{31}{2013}{{Cotter et~al.}}{{Cotter, Roberts, Stuart, and White}}}
\bibcite{cotter2009bayesian}{{32}{2009}{{Cotter et~al.}}{{Cotter, Dashti, Robinson, and Stuart}}}
\bibcite{damianou2013deep}{{33}{2013}{{Damianou and Lawrence}}{{}}}
\bibcite{de2022cost}{{34}{2022}{{De~Hoop et~al.}}{{De~Hoop, Huang, Qian, and Stuart}}}
\bibcite{devlin2018bert}{{35}{2018}{{Devlin et~al.}}{{Devlin, Chang, Lee, and Toutanova}}}
\bibcite{devore1998nonlinear}{{36}{1998}{{DeVore}}{{}}}
\bibcite{DeVoreReducedBasis}{{37}{2014}{{DeVore}}{{}}}
\bibcite{dosovitskiy2020image}{{38}{2020}{{Dosovitskiy et~al.}}{{Dosovitskiy, Beyer, Kolesnikov, Weissenborn, Zhai, Unterthiner, Dehghani, Minderer, Heigold, Gelly, et~al.}}}
\bibcite{dudley2011concrete}{{39}{2011}{{Dudley and Norvaisa}}{{}}}
\bibcite{dudley2010concrete}{{40}{2010}{{Dudley and Norvai{\v {s}}a}}{{}}}
\bibcite{dugundji1961anextension}{{41}{1951}{{Dugundji}}{{}}}
\bibcite{aretha}{{42}{2018}{{Dunlop et~al.}}{{Dunlop, Girolami, Stuart, and Teckentrup}}}
\bibcite{weinan2017proposal}{{43}{2017}{{E}}{{}}}
\bibcite{e2011principles}{{44}{2011}{{E}}{{}}}
\bibcite{Weinan}{{45}{2018}{{E and Yu}}{{}}}
\bibcite{fan2019bcr}{{46}{2019{a}}{{Fan et~al.}}{{Fan, Bohorquez, and Ying}}}
\bibcite{fan2019multiscale2}{{47}{2019{b}}{{Fan et~al.}}{{Fan, Feliu-Faba, Lin, Ying, and Zepeda-N{\'u}nez}}}
\bibcite{fan2019multiscale}{{48}{2019{c}}{{Fan et~al.}}{{Fan, Lin, Ying, and Zepeda-N{\'u}nez}}}
\bibcite{fefferman2007cmextension}{{49}{2007}{{Fefferman}}{{}}}
\bibcite{fresca2022poddlrom}{{50}{2022}{{Fresca and Manzoni}}{{}}}
\bibcite{gardner2018product}{{51}{2018}{{Gardner et~al.}}{{Gardner, Pleiss, Wu, Weinberger, and Wilson}}}
\bibcite{Garriga-AlonsoGP}{{52}{2018}{{{Garriga-Alonso} et~al.}}{{{Garriga-Alonso}, {Rasmussen}, and {Aitchison}}}}
\bibcite{gilmer2017neural}{{53}{2017}{{Gilmer et~al.}}{{Gilmer, Schoenholz, Riley, Vinyals, and Dahl}}}
\bibcite{GlobersonLivni}{{54}{2016}{{Globerson and Livni}}{{}}}
\bibcite{greenfeld2019learning}{{55}{2019}{{Greenfeld et~al.}}{{Greenfeld, Galun, Basri, Yavneh, and Kimmel}}}
\bibcite{greengard1997new}{{56}{1997}{{Greengard and Rokhlin}}{{}}}
\bibcite{grothendieck1955produits}{{57}{1955}{{Grothendieck}}{{}}}
\bibcite{guibas2021adaptive}{{58}{2021}{{Guibas et~al.}}{{Guibas, Mardani, Li, Tao, Anandkumar, and Catanzaro}}}
\bibcite{guo2016convolutional}{{59}{2016}{{Guo et~al.}}{{Guo, Li, and Iorio}}}
\bibcite{gurtin1982introduction}{{60}{1982}{{Gurtin}}{{}}}
\bibcite{Guss}{{61}{2016}{{Guss}}{{}}}
\bibcite{haber2017stable}{{62}{2017}{{Haber and Ruthotto}}{{}}}
\bibcite{hamilton2017inductive}{{63}{2017}{{Hamilton et~al.}}{{Hamilton, Ying, and Leskovec}}}
\bibcite{he2019mgnet}{{64}{2019}{{He and Xu}}{{}}}
\bibcite{he2016deep}{{65}{2016}{{He et~al.}}{{He, Zhang, Ren, and Sun}}}
\bibcite{herrmann2020deep}{{66}{2020}{{Herrmann et~al.}}{{Herrmann, Schwab, and Zech}}}
\bibcite{hornik1989multilayer}{{67}{1989}{{Hornik et~al.}}{{Hornik, Stinchcombe, White, et~al.}}}
\bibcite{jiang2020meshfreeflownet}{{68}{2020}{{Jiang et~al.}}{{Jiang, Esmaeilzadeh, Azizzadenesheli, Kashinath, Mustafa, Tchelepi, Marcus, Anandkumar, et~al.}}}
\bibcite{johnson2012numerical}{{69}{2012}{{Johnson}}{{}}}
\bibcite{kashinath2020enforcing}{{70}{2020}{{Kashinath et~al.}}{{Kashinath, Marcus, et~al.}}}
\bibcite{khoo2019switchnet}{{71}{2019}{{Khoo and Ying}}{{}}}
\bibcite{khoo2017solving}{{72}{2021}{{Khoo et~al.}}{{Khoo, Lu, and Ying}}}
\bibcite{kipf2016semi}{{73}{2016}{{Kipf and Welling}}{{}}}
\bibcite{kondor2014multiresolution}{{74}{2014}{{Kondor et~al.}}{{Kondor, Teneva, and Garg}}}
\bibcite{kovachki2021universal}{{75}{2021}{{Kovachki et~al.}}{{Kovachki, Lanthaler, and Mishra}}}
\bibcite{kraichnan67inertial}{{76}{1967}{{Kraichnan}}{{}}}
\bibcite{kulis2006learning}{{77}{2006}{{Kulis et~al.}}{{Kulis, Sustik, and Dhillon}}}
\bibcite{kutyniok2022atheoretical}{{78}{2022}{{Kutyniok et~al.}}{{Kutyniok, Petersen, Raslan, and Schneider}}}
\bibcite{lan2017low}{{79}{2017}{{Lan et~al.}}{{Lan, Zhang, Ge, Cheng, Liu, Rauber, Li, Wang, and Zha}}}
\bibcite{lanthaler2021error}{{80}{2021}{{Lanthaler et~al.}}{{Lanthaler, Mishra, and Karniadakis}}}
\bibcite{leoni2009first}{{81}{2009}{{Leoni}}{{}}}
\bibcite{li2020fourier}{{82}{2020{a}}{{Li et~al.}}{{Li, Kovachki, Azizzadenesheli, Liu, Bhattacharya, Stuart, and Anandkumar}}}
\bibcite{li2020multipole}{{83}{2020{b}}{{Li et~al.}}{{Li, Kovachki, Azizzadenesheli, Liu, Bhattacharya, Stuart, and Anandkumar}}}
\bibcite{li2020neural}{{84}{2020{c}}{{Li et~al.}}{{Li, Kovachki, Azizzadenesheli, Liu, Bhattacharya, Stuart, and Anandkumar}}}
\bibcite{li2021physics}{{85}{2021}{{Li et~al.}}{{Li, Zheng, Kovachki, Jin, Chen, Liu, Azizzadenesheli, and Anandkumar}}}
\bibcite{lu2019deeponet}{{86}{2019}{{Lu et~al.}}{{Lu, Jin, and Karniadakis}}}
\bibcite{lu2021learning}{{87}{2021{a}}{{Lu et~al.}}{{Lu, Jin, Pang, Zhang, and Karniadakis}}}
\bibcite{lu2021comprehensive}{{88}{2021{b}}{{Lu et~al.}}{{Lu, Meng, Cai, Mao, Goswami, Zhang, and Karniadakis}}}
\bibcite{mathieu2013fast}{{89}{2013}{{Mathieu et~al.}}{{Mathieu, Henaff, and LeCun}}}
\bibcite{MathewsGP}{{90}{2018}{{{Matthews} et~al.}}{{{Matthews}, {Rowland}, {Hron}, {Turner}, and {Ghahramani}}}}
\bibcite{mingo2004Fourier}{{91}{2004}{{Mingo et~al.}}{{Mingo, Aslanyan, Castellanos, Diaz, and Riazanov}}}
\bibcite{murphy2018janossy}{{92}{2018}{{Murphy et~al.}}{{Murphy, Srinivasan, Rao, and Ribeiro}}}
\bibcite{Neal}{{93}{1996}{{Neal}}{{}}}
\bibcite{nelsen2020random}{{94}{2021}{{Nelsen and Stuart}}{{}}}
\bibcite{nystrom1930praktische}{{95}{1930}{{Nystr{\"o}m}}{{}}}
\bibcite{o2020derivative}{{96}{2020}{{O'Leary-Roseberry et~al.}}{{O'Leary-Roseberry, Villa, Chen, and Ghattas}}}
\bibcite{opschoor2020deep}{{97}{2020}{{Opschoor et~al.}}{{Opschoor, Schwab, and Zech}}}
\bibcite{pan2020physics}{{98}{2020}{{Pan and Duraisamy}}{{}}}
\bibcite{pathak2020using}{{99}{2020}{{Pathak et~al.}}{{Pathak, Mustafa, Kashinath, Motheau, Kurth, and Day}}}
\bibcite{pelczynski2001contribution}{{100}{2001}{{Pełczyński and Wojciechowski}}{{}}}
\bibcite{pfaff2020learning}{{101}{2020}{{Pfaff et~al.}}{{Pfaff, Fortunato, Sanchez-Gonzalez, and Battaglia}}}
\bibcite{pinkus1985nwidths}{{102}{1985}{{Pinkus}}{{}}}
\bibcite{pinkus1999approximation}{{103}{1999}{{Pinkus}}{{}}}
\bibcite{poole2016exponential}{{104}{2016}{{Poole et~al.}}{{Poole, Lahiri, Raghu, Sohl-Dickstein, and Ganguli}}}
\bibcite{quinonero2005aunifying}{{105}{2005}{{Qui\~{n}onero Candela and Rasmussen}}{{}}}
\bibcite{rahimi2008uniform}{{106}{2008}{{Rahimi and Recht}}{{}}}
\bibcite{raissi2019physics}{{107}{2019}{{Raissi et~al.}}{{Raissi, Perdikaris, and Karniadakis}}}
\bibcite{ronneberger2015u}{{108}{2015}{{Ronneberger et~al.}}{{Ronneberger, Fischer, and Brox}}}
\bibcite{BengioLeRoux}{{109}{2007}{{Roux and Bengio}}{{}}}
\bibcite{scarselli2008graph}{{110}{2008}{{Scarselli et~al.}}{{Scarselli, Gori, Tsoi, Hagenbuchner, and Monfardini}}}
\bibcite{schwab2019deep}{{111}{2019}{{Schwab and Zech}}{{}}}
\bibcite{sirignano2018dgm}{{112}{2018}{{Sirignano and Spiliopoulos}}{{}}}
\bibcite{sitzmann2020implicit}{{113}{2020}{{Sitzmann et~al.}}{{Sitzmann, Martel, Bergman, Lindell, and Wetzstein}}}
\bibcite{smith2020eikonet}{{114}{2020}{{Smith et~al.}}{{Smith, Azizzadenesheli, and Ross}}}
\bibcite{stein1970singular}{{115}{1970}{{Stein}}{{}}}
\bibcite{stuart_2010}{{116}{2010}{{Stuart}}{{}}}
\bibcite{trefethen2000spectral}{{117}{2000}{{Trefethen}}{{}}}
\bibcite{trillos2018variational}{{118}{2018}{{Trillos and Slep{\v {c}}ev}}{{}}}
\bibcite{trillos2020error}{{119}{2020}{{Trillos et~al.}}{{Trillos, Gerlach, Hein, and Slep{\v {c}}ev}}}
\bibcite{um2020solver}{{120}{2020{a}}{{Um et~al.}}{{Um, Holl, Brand, Thuerey, et~al.}}}
\bibcite{um2020solverintheloop}{{121}{2020{b}}{{Um et~al.}}{{Um, Raymond, Fei, Holl, Brand, and Thuerey}}}
\bibcite{ummenhofer2020lagrangian}{{122}{2020}{{Ummenhofer et~al.}}{{Ummenhofer, Prantl, Th{\"u}rey, and Koltun}}}
\bibcite{Vapnik1998}{{123}{1998}{{Vapnik}}{{}}}
\bibcite{vaswani2017attention}{{124}{2017}{{Vaswani et~al.}}{{Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez, Kaiser, and Polosukhin}}}
\bibcite{velivckovic2017graph}{{125}{2017}{{Veli{\v {c}}kovi{\'c} et~al.}}{{Veli{\v {c}}kovi{\'c}, Cucurull, Casanova, Romero, Lio, and Bengio}}}
\bibcite{von2008consistency}{{126}{2008}{{Von~Luxburg et~al.}}{{Von~Luxburg, Belkin, and Bousquet}}}
\bibcite{wang2020towards}{{127}{2020}{{Wang et~al.}}{{Wang, Kashinath, Mustafa, Albert, and Yu}}}
\bibcite{wang2021learning}{{128}{2021}{{Wang et~al.}}{{Wang, Wang, and Perdikaris}}}
\bibcite{wen2021u}{{129}{2021}{{Wen et~al.}}{{Wen, Li, Azizzadenesheli, Anandkumar, and Benson}}}
\bibcite{whitney1934functions}{{130}{1934}{{Whitney}}{{}}}
\bibcite{Williams}{{131}{1996}{{Williams}}{{}}}
\bibcite{zhu2021long}{{132}{2021}{{Zhu et~al.}}{{Zhu, Ping, Xiao, Shoeybi, Goldstein, Anandkumar, and Catanzaro}}}
\bibcite{Zabaras}{{133}{2018}{{Zhu and Zabaras}}{{}}}
\@writefile{toc}{\contentsline {section}{\numberline {A}}{97}{section.1}\protected@file@percent }
\newlabel{sec:appendix_notation}{{A}{97}{Acknowledgements}{section.1}{}}
\@writefile{lot}{\contentsline {table}{\numberline {10}{\ignorespaces Table of notations: operator learning and neural operators}}{97}{table.caption.27}\protected@file@percent }
\newlabel{table:notations1}{{10}{97}{Table of notations: operator learning and neural operators}{table.caption.27}{}}
\citation{whitney1934functions,brudnyi2012methods}
\citation{adams2003sobolev}
\@writefile{toc}{\contentsline {section}{\numberline {B}}{99}{section.2}\protected@file@percent }
\newlabel{sec:appendix_approximationproperty}{{B}{99}{Acknowledgements}{section.2}{}}
\newlabel{def:schauder_bases}{{15}{99}{Acknowledgements}{theorem.15}{}}
\citation{albiac2006topics}
\citation{albiac2006topics}
\newlabel{def:finiterank}{{16}{100}{Acknowledgements}{theorem.16}{}}
\newlabel{def:ap}{{17}{100}{Acknowledgements}{theorem.17}{}}
\newlabel{lemma:schauder_ap}{{18}{100}{Acknowledgements}{theorem.18}{}}
\newlabel{lemma:schauder_schauder}{{19}{101}{Acknowledgements}{theorem.19}{}}
\newlabel{lemma:ap_ap}{{20}{102}{Acknowledgements}{theorem.20}{}}
\newlabel{lemma:compact_union}{{21}{102}{Acknowledgements}{theorem.21}{}}
\citation{Kovachki}
\newlabel{lemma:finitedim_approx}{{22}{104}{Acknowledgements}{theorem.22}{}}
\newlabel{lemma:cm_isomorphism}{{23}{106}{Acknowledgements}{theorem.23}{}}
\newlabel{corr:cm_isomorphism}{{24}{106}{Acknowledgements}{theorem.24}{}}
\newlabel{eq:tau}{{48}{107}{Acknowledgements}{equation.48}{}}
\newlabel{lemma:w1_isomorphism}{{25}{107}{Acknowledgements}{theorem.25}{}}
\citation{albiac2006topics}
\citation{stein1970singular}
\citation{pelczynski2001contribution}
\citation{albiac2006topics}
\citation{ciesielski1972construction}
\citation{fefferman2007cmextension}
\newlabel{lemma:ap}{{26}{108}{Acknowledgements}{theorem.26}{}}
\citation{ciesielski1972construction}
\citation{stein1970singular}
\citation{pelczynski2001contribution}
\citation{leoni2009first}
\@writefile{toc}{\contentsline {section}{\numberline {C}}{110}{section.3}\protected@file@percent }
\newlabel{sec:appendix_functionalapprox}{{C}{110}{Acknowledgements}{section.3}{}}
\newlabel{lemma:reisz}{{27}{110}{Acknowledgements}{theorem.27}{}}
\citation{leoni2009first}
\citation{conway2007acourse}
\citation{adams2003sobolev}
\newlabel{lemma:wmp_kernelapprox}{{28}{111}{Acknowledgements}{theorem.28}{}}
\citation{leoni2009first}
\citation{adams2003sobolev}
\citation{bogachev2007measure}
\newlabel{lemma:cm_delta_approx}{{29}{115}{Acknowledgements}{theorem.29}{}}
\newlabel{lemma:c_kernelapprox}{{30}{117}{Acknowledgements}{theorem.30}{}}
\newlabel{lemma:cm_kernelapprox}{{31}{118}{Acknowledgements}{theorem.31}{}}
\@writefile{toc}{\contentsline {section}{\numberline {D}}{119}{section.4}\protected@file@percent }
\newlabel{sec:appendix_nos}{{D}{119}{Acknowledgements}{section.4}{}}
\newlabel{lemma:input_approx}{{32}{119}{Acknowledgements}{theorem.32}{}}
\newlabel{lemma:cm_input_approx}{{33}{120}{Acknowledgements}{theorem.33}{}}
\citation{leoni2009first}
\newlabel{lemma:output_approx}{{34}{121}{Acknowledgements}{theorem.34}{}}
\newlabel{lemma:nn_emulation}{{35}{123}{Acknowledgements}{theorem.35}{}}
\@writefile{toc}{\contentsline {section}{\numberline {E}}{123}{section.5}\protected@file@percent }
\newlabel{sec_proof:discretizational_invariance}{{E}{123}{Acknowledgements}{section.5}{}}
\newlabel{eq:disc_inv_modulusofcont}{{49}{124}{Acknowledgements}{equation.49}{}}
\newlabel{eq:disc_inv_needtoshow}{{50}{124}{Acknowledgements}{equation.50}{}}
\citation{pinkus1999approximation}
\@writefile{toc}{\contentsline {section}{\numberline {F}}{126}{section.6}\protected@file@percent }
\newlabel{sec_proof:main_compact}{{F}{126}{Acknowledgements}{section.6}{}}
\citation{aaronson1997introduction}
\citation{dugundji1961anextension}
\@writefile{toc}{\contentsline {section}{\numberline {G}}{129}{section.7}\protected@file@percent }
\newlabel{sec_proof:measurable_approx}{{G}{129}{Acknowledgements}{section.7}{}}
\newlabel{LastPage}{{G}{130}{Acknowledgements}{page.130}{}}
\gdef\lastpage@lastpage{130}
\gdef\lastpage@lastpageHy{130}
\gdef \@abspage@last{130}
