\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand*\HyPL@Entry[1]{}
\bibstyle{plainnat}
\HyPL@Entry{0<</S/D>>}
\citation{scarselli2008graph}
\citation{vaswani2017attention}
\@writefile{toc}{\contentsline {section}{\numberline {1}简介}{4}{section.1}\protected@file@percent }
\newlabel{sec:I}{{1}{4}{\contentsname \@mkboth {\MakeUppercase \contentsname }{\MakeUppercase \contentsname }}{section.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}我们的方法}{4}{subsection.1.1}\protected@file@percent }
\newlabel{ssec:OC}{{1.1}{4}{\contentsname \@mkboth {\MakeUppercase \contentsname }{\MakeUppercase \contentsname }}{subsection.1.1}{}}
\citation{hornik1989multilayer}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Discretization Invariance}}{5}{figure.caption.1}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:discretization-invariance}{{1}{5}{Discretization Invariance}{figure.caption.1}{}}
\citation{chandler2013invariant}
\citation{gurtin1982introduction}
\citation{johnson2012numerical}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}背景与相关工作}{6}{subsection.1.2}\protected@file@percent }
\newlabel{ssec:LR}{{1.2}{6}{\contentsname \@mkboth {\MakeUppercase \contentsname }{\MakeUppercase \contentsname }}{subsection.1.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Neural operator architecture schematic}}{7}{figure.caption.2}\protected@file@percent }
\newlabel{fig:NO_architecture}{{2}{7}{Neural operator architecture schematic}{figure.caption.2}{}}
\citation{belongie2002spectral}
\citation{von2008consistency,trillos2018variational,trillos2020error}
\citation{boyd2001chebyshev,trefethen2000spectral}
\citation{raissi2019physics}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces 深度学习模型的比较。第一行表示该模型是否具有离散化不变性。第二行和第三行表示输出和输入是否为函数。第四行表示该模型类是否为算子的通用逼近器。神经算子是离散化不变的深度学习方法，能够输出函数并逼近任意算子。}}{8}{table.caption.3}\protected@file@percent }
\newlabel{table:deeplearning_comparison}{{1}{8}{深度学习模型的比较。第一行表示该模型是否具有离散化不变性。第二行和第三行表示输出和输入是否为函数。第四行表示该模型类是否为算子的通用逼近器。神经算子是离散化不变的深度学习方法，能够输出函数并逼近任意算子。}{table.caption.3}{}}
\citation{lu2019deeponet}
\citation{de2022cost}
\citation{vaswani2017attention}
\citation{dosovitskiy2020image}
\citation{lu2019deeponet}
\@writefile{toc}{\contentsline {section}{\numberline {2}Learning Operators(算子学习设定)}{10}{section.2}\protected@file@percent }
\newlabel{sec:setting}{{2}{10}{\contentsname \@mkboth {\MakeUppercase \contentsname }{\MakeUppercase \contentsname }}{section.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Generic Parametric PDEs (泛型参数化偏微分方程)}{10}{subsection.2.1}\protected@file@percent }
\newlabel{sec:genericPDE}{{2.1}{10}{\contentsname \@mkboth {\MakeUppercase \contentsname }{\MakeUppercase \contentsname }}{subsection.2.1}{}}
\newlabel{eq:generalpde0}{{1}{10}{\contentsname \@mkboth {\MakeUppercase \contentsname }{\MakeUppercase \contentsname }}{equation.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}问题设定}{10}{subsection.2.2}\protected@file@percent }
\newlabel{sec:PS}{{2.2}{10}{\contentsname \@mkboth {\MakeUppercase \contentsname }{\MakeUppercase \contentsname }}{subsection.2.2}{}}
\newlabel{eq:approxmap2}{{2}{10}{\contentsname \@mkboth {\MakeUppercase \contentsname }{\MakeUppercase \contentsname }}{equation.2}{}}
\citation{Vapnik1998}
\citation{hornik1989multilayer}
\newlabel{eq:bochner_error}{{3}{11}{\contentsname \@mkboth {\MakeUppercase \contentsname }{\MakeUppercase \contentsname }}{equation.3}{}}
\newlabel{eq:empirical_risk}{{4}{11}{\contentsname \@mkboth {\MakeUppercase \contentsname }{\MakeUppercase \contentsname }}{equation.4}{}}
\newlabel{eq:unform_risk}{{5}{11}{\contentsname \@mkboth {\MakeUppercase \contentsname }{\MakeUppercase \contentsname }}{equation.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Discretization(离散化)}{11}{subsection.2.3}\protected@file@percent }
\newlabel{sec:discretization}{{2.3}{11}{\contentsname \@mkboth {\MakeUppercase \contentsname }{\MakeUppercase \contentsname }}{subsection.2.3}{}}
\newlabel{def:discretization_invariance}{{4}{12}{\contentsname \@mkboth {\MakeUppercase \contentsname }{\MakeUppercase \contentsname }}{theorem.4}{}}
\citation{dudley2011concrete}
\@writefile{toc}{\contentsline {section}{\numberline {3}Neural Operators(神经算子)}{14}{section.3}\protected@file@percent }
\newlabel{sec:neuraloperators}{{3}{14}{\contentsname \@mkboth {\MakeUppercase \contentsname }{\MakeUppercase \contentsname }}{section.3}{}}
\newlabel{eq:F}{{6}{14}{\contentsname \@mkboth {\MakeUppercase \contentsname }{\MakeUppercase \contentsname }}{equation.6}{}}
\citation{he2016deep}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}积分核算子（Integral Kernel Operators）}{15}{subsection.3.1}\protected@file@percent }
\newlabel{eq:kernelop1}{{7}{15}{\contentsname \@mkboth {\MakeUppercase \contentsname }{\MakeUppercase \contentsname }}{equation.7}{}}
\citation{vaswani2017attention}
\newlabel{eq:kernelop2}{{8}{16}{\contentsname \@mkboth {\MakeUppercase \contentsname }{\MakeUppercase \contentsname }}{equation.8}{}}
\newlabel{eq:kernelop3}{{9}{16}{\contentsname \@mkboth {\MakeUppercase \contentsname }{\MakeUppercase \contentsname }}{equation.9}{}}
\newlabel{eq:kernelop1}{{10}{16}{\contentsname \@mkboth {\MakeUppercase \contentsname }{\MakeUppercase \contentsname }}{equation.10}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}单隐藏层构造 (Single Hidden Layer Construction)}{16}{subsection.3.2}\protected@file@percent }
\newlabel{eq:onelayer}{{11}{16}{\contentsname \@mkboth {\MakeUppercase \contentsname }{\MakeUppercase \contentsname }}{equation.11}{}}
\newlabel{eq:singlehiddenlayer}{{12}{17}{\contentsname \@mkboth {\MakeUppercase \contentsname }{\MakeUppercase \contentsname }}{equation.12}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}预处理（Preprocessing）}{17}{subsection.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}离散化不变性与逼近性（Discretization Invariance and Approximation）}{17}{subsection.3.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4}参数化与计算 (Parameterization and Computation)}{19}{section.4}\protected@file@percent }
\newlabel{sec:four_schemes}{{4}{19}{\contentsname \@mkboth {\MakeUppercase \contentsname }{\MakeUppercase \contentsname }}{section.4}{}}
\newlabel{eq:onelayercompute}{{13}{19}{\contentsname \@mkboth {\MakeUppercase \contentsname }{\MakeUppercase \contentsname }}{equation.13}{}}
\newlabel{eq:onelayerlinear}{{14}{19}{\contentsname \@mkboth {\MakeUppercase \contentsname }{\MakeUppercase \contentsname }}{equation.14}{}}
\citation{li2020neural}
\newlabel{eq:onelayerlinear_MC}{{15}{20}{\contentsname \@mkboth {\MakeUppercase \contentsname }{\MakeUppercase \contentsname }}{equation.15}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}核矩阵（Kernel Matrix）}{20}{subsection.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}图神经算子（Graph Neural Operator, GNO）}{20}{subsection.4.2}\protected@file@percent }
\newlabel{sec:graphneuraloperator}{{4.2}{20}{\contentsname \@mkboth {\MakeUppercase \contentsname }{\MakeUppercase \contentsname }}{subsection.4.2}{}}
\newlabel{eq:nystrom_matrix}{{16}{21}{\contentsname \@mkboth {\MakeUppercase \contentsname }{\MakeUppercase \contentsname }}{equation.16}{}}
\newlabel{eq:onelayertruncation}{{17}{21}{截断（Truncation）}{equation.17}{}}
\citation{gilmer2017neural}
\citation{gilmer2017neural}
\citation{pfaff2020learning}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}卷积神经网络（Convolutional Neural Networks）.}{23}{subsection.4.3}\protected@file@percent }
\newlabel{eq:onelayercnn}{{18}{23}{图神经网络（Graph Neural Networks）}{equation.18}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}Low-rank Neural Operator (LNO)（低秩神经算子）}{24}{subsection.4.4}\protected@file@percent }
\newlabel{sec:lowrank}{{4.4}{24}{图神经网络（Graph Neural Networks）}{subsection.4.4}{}}
\citation{lu2019deeponet}
\newlabel{eq:finiteranksvd}{{19}{25}{图神经网络（Graph Neural Networks）}{equation.19}{}}
\citation{lu2019deeponet}
\citation{e2011principles}
\citation{li2020multipole}
\newlabel{eq:finiteranksvd}{{20}{26}{图神经网络（Graph Neural Networks）}{equation.20}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5}Multipole Graph Neural Operator (MGNO)}{26}{subsection.4.5}\protected@file@percent }
\newlabel{sec:multipole}{{4.5}{26}{图神经网络（Graph Neural Networks）}{subsection.4.5}{}}
\newlabel{eq:decomposition_matrix}{{21}{27}{图神经网络（Graph Neural Networks）}{equation.21}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Hierarchical matrix decomposition}}{27}{figure.caption.6}\protected@file@percent }
\newlabel{fig:hmatrix}{{3}{27}{Hierarchical matrix decomposition}{figure.caption.6}{}}
\citation{greengard1997new}
\citation{quinonero2005aunifying}
\citation{kondor2014multiresolution}
\newlabel{eq:hierarchy_decomposition}{{22}{28}{图神经网络（Graph Neural Networks）}{equation.22}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces V-cycle}}{29}{figure.caption.7}\protected@file@percent }
\newlabel{fig:vcycle}{{4}{29}{V-cycle}{figure.caption.7}{}}
\newlabel{fig:multigraph}{{4}{29}{V-cycle}{figure.caption.7}{}}
\newlabel{eq:all1}{{23}{29}{图神经网络（Graph Neural Networks）}{equation.23}{}}
\newlabel{eq:all2}{{24}{29}{图神经网络（Graph Neural Networks）}{equation.24}{}}
\newlabel{eq:all3}{{25}{29}{图神经网络（Graph Neural Networks）}{equation.25}{}}
\citation{greengard1997new}
\newlabel{eq:up}{{26}{30}{图神经网络（Graph Neural Networks）}{equation.26}{}}
\newlabel{eq:down}{{27}{30}{图神经网络（Graph Neural Networks）}{equation.27}{}}
\citation{nelsen2020random}
\citation{li2020fourier}
\citation{kovachki2021universal}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.6}Fourier Neural Operator (FNO)}{31}{subsection.4.6}\protected@file@percent }
\newlabel{sec:fourier}{{4.6}{31}{图神经网络（Graph Neural Networks）}{subsection.4.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces  {\bf  top:} The architecture of the neural operators; \textbf  {bottom:} Fourier layer.}}{32}{figure.caption.8}\protected@file@percent }
\newlabel{fig:arch}{{5}{32}{{\bf top:} The architecture of the neural operators; \textbf {bottom:} Fourier layer}{figure.caption.8}{}}
\newlabel{eq:fourierlayer}{{28}{32}{图神经网络（Graph Neural Networks）}{equation.28}{}}
\newlabel{eq:fft_mult}{{29}{33}{图神经网络（Graph Neural Networks）}{equation.29}{}}
\citation{bruno2007accurate}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.7}Summary}{35}{subsection.4.7}\protected@file@percent }
\citation{lu2019deeponet}
\@writefile{toc}{\contentsline {section}{\numberline {5}Neural Operators and Other Deep Learning Models}{36}{section.5}\protected@file@percent }
\newlabel{sec:framework}{{5}{36}{图神经网络（Graph Neural Networks）}{section.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}DeepONets}{36}{subsection.5.1}\protected@file@percent }
\newlabel{sec:deeponets}{{5.1}{36}{图神经网络（Graph Neural Networks）}{subsection.5.1}{}}
\newlabel{prop:deeponet}{{5}{37}{图神经网络（Graph Neural Networks）}{theorem.5}{}}
\citation{lanthaler2021error}
\citation{de2022cost}
\newlabel{eq:likedeeponet}{{34}{38}{图神经网络（Graph Neural Networks）}{equation.34}{}}
\newlabel{eq:likedeeponet-operator}{{35}{38}{图神经网络（Graph Neural Networks）}{equation.35}{}}
\citation{devore1998nonlinear}
\citation{pinkus1985nwidths}
\citation{cohendevore}
\citation{devore1998nonlinear}
\citation{bonito2020nonlinear,cohen2020optimal}
\citation{Kovachki}
\citation{lu2019deeponet}
\citation{lu2021comprehensive}
\citation{Kovachki}
\citation{box1976science}
\citation{lu2021comprehensive}
\citation{lu2019deeponet}
\citation{lu2021comprehensive}
\citation{lu2021comprehensive}
\citation{bruno2007accurate}
\citation{vaswani2017attention}
\citation{devlin2018bert,brown2020language}
\citation{dosovitskiy2020image}
\citation{vaswani2017attention}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Transformers as a Special Case of Neural Operators}{41}{subsection.5.2}\protected@file@percent }
\newlabel{sec:transformers}{{5.2}{41}{图神经网络（Graph Neural Networks）}{subsection.5.2}{}}
\newlabel{prop:attention}{{6}{41}{图神经网络（Graph Neural Networks）}{theorem.6}{}}
\newlabel{eq:liketransformer}{{36}{41}{图神经网络（Graph Neural Networks）}{equation.36}{}}
\newlabel{eq:continoustransformer}{{37}{42}{图神经网络（Graph Neural Networks）}{equation.37}{}}
\newlabel{eq:discretetransformer}{{38}{42}{图神经网络（Graph Neural Networks）}{equation.38}{}}
\citation{ba2016layer}
\citation{zhu2021long}
\citation{guibas2021adaptive}
\citation{choromanski2020rethinking,dosovitskiy2020image}
\@writefile{toc}{\contentsline {section}{\numberline {6}Test Problems}{44}{section.6}\protected@file@percent }
\newlabel{sec:problems}{{6}{44}{图神经网络（Graph Neural Networks）}{section.6}{}}
\newlabel{eq:generalpde}{{39}{44}{图神经网络（Graph Neural Networks）}{equation.39}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}Poisson Equation}{44}{subsection.6.1}\protected@file@percent }
\newlabel{ssec:poisson}{{6.1}{44}{图神经网络（Graph Neural Networks）}{subsection.6.1}{}}
\newlabel{eq:poisson}{{40}{44}{图神经网络（Graph Neural Networks）}{equation.40}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2}Darcy Flow}{46}{subsection.6.2}\protected@file@percent }
\newlabel{ssec:darcy}{{6.2}{46}{图神经网络（Graph Neural Networks）}{subsection.6.2}{}}
\newlabel{eq:darcy}{{41}{46}{图神经网络（Graph Neural Networks）}{equation.41}{}}
\newlabel{eq:darcysolutionop}{{42}{46}{图神经网络（Graph Neural Networks）}{equation.42}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3}Burgers' Equation}{47}{subsection.6.3}\protected@file@percent }
\newlabel{ssec:burgers}{{6.3}{47}{图神经网络（Graph Neural Networks）}{subsection.6.3}{}}
\newlabel{eq:burgers}{{43}{47}{图神经网络（Graph Neural Networks）}{equation.43}{}}
\newlabel{eq:burgerssolutionop}{{44}{47}{图神经网络（Graph Neural Networks）}{equation.44}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.4}Navier-Stokes Equation}{47}{subsection.6.4}\protected@file@percent }
\newlabel{ssec:nse}{{6.4}{47}{图神经网络（Graph Neural Networks）}{subsection.6.4}{}}
\newlabel{eq:navierstokes-velocity}{{45}{47}{图神经网络（Graph Neural Networks）}{equation.45}{}}
\citation{chandler2013invariant}
\newlabel{eq:navierstokes}{{46}{48}{图神经网络（Graph Neural Networks）}{equation.46}{}}
\newlabel{eq:nssolutionop_add}{{47}{48}{图神经网络（Graph Neural Networks）}{equation.47}{}}
\newlabel{eq:nssolutionop2}{{48}{48}{图神经网络（Graph Neural Networks）}{equation.48}{}}
\citation{chandler2013invariant}
\citation{cotter2009bayesian,stuart_2010}
\citation{Cotter_2013}
\citation{Cotter_2013}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.4.1}Bayesian Inverse Problem}{49}{subsubsection.6.4.1}\protected@file@percent }
\newlabel{sec:problem:bayesian}{{6.4.1}{49}{图神经网络（Graph Neural Networks）}{subsubsection.6.4.1}{}}
\newlabel{eq:inverseproblem}{{49}{49}{图神经网络（Graph Neural Networks）}{equation.49}{}}
\citation{kraichnan67inertial}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.4.2}Spectra}{50}{subsubsection.6.4.2}\protected@file@percent }
\newlabel{app:sepctral}{{6.4.2}{50}{图神经网络（Graph Neural Networks）}{subsubsection.6.4.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Spectral Decay of Navier-Stokes.}}{50}{figure.caption.9}\protected@file@percent }
\newlabel{fig:spectral1}{{6}{50}{Spectral Decay of Navier-Stokes}{figure.caption.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.5}Choice of Loss Criteria}{50}{subsection.6.5}\protected@file@percent }
\citation{de2022cost}
\@writefile{toc}{\contentsline {section}{\numberline {7}Numerical Results}{51}{section.7}\protected@file@percent }
\newlabel{sec:numerics}{{7}{51}{图神经网络（Graph Neural Networks）}{section.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1}Poisson Equation}{52}{subsection.7.1}\protected@file@percent }
\newlabel{ssec:result_green}{{7.1}{52}{图神经网络（Graph Neural Networks）}{subsection.7.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Kernel for one-dimensional Green's function, with the Nystrom approximation method}}{53}{figure.caption.10}\protected@file@percent }
\newlabel{fig:kernel1d_nystrom}{{7}{53}{Kernel for one-dimensional Green's function, with the Nystrom approximation method}{figure.caption.10}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2}Darcy and Burgers Equations}{53}{subsection.7.2}\protected@file@percent }
\newlabel{ssec:darcyburgers}{{7.2}{53}{图神经网络（Graph Neural Networks）}{subsection.7.2}{}}
\citation{Zabaras}
\citation{Kovachki}
\citation{DeVoreReducedBasis}
\citation{lu2019deeponet}
\citation{lanthaler2021error}
\citation{lu2019deeponet}
\citation{Kovachki}
\citation{lu2021comprehensive}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.2.1}Darcy Flow}{54}{subsubsection.7.2.1}\protected@file@percent }
\newlabel{ssec:DF}{{7.2.1}{54}{图神经网络（Graph Neural Networks）}{subsubsection.7.2.1}{}}
\citation{lu2021comprehensive}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Benchmark on Burger's equation and Darcy Flow {}}}{55}{figure.caption.11}\protected@file@percent }
\newlabel{fig:error}{{8}{55}{Benchmark on Burger's equation and Darcy Flow \done {It is confusing to have the two left hand panels with the right-most panel; the left most panels who resolution invariance very clear;y, the NSE panel is somewhat different. I suggest separate figures.}}{figure.caption.11}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.2.2}Burgers' Equation}{55}{subsubsection.7.2.2}\protected@file@percent }
\newlabel{ssec:BE}{{7.2.2}{55}{图神经网络（Graph Neural Networks）}{subsubsection.7.2.2}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Relative error on 2-d Darcy Flow for different resolutions $s$.}}{56}{table.caption.12}\protected@file@percent }
\newlabel{table:darcy}{{2}{56}{Relative error on 2-d Darcy Flow for different resolutions $s$}{table.caption.12}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Darcy, trained on $16 \times 16$, tested on $241 \times 241$}}{56}{figure.caption.14}\protected@file@percent }
\newlabel{fig:super1}{{9}{56}{Darcy, trained on $16 \times 16$, tested on $241 \times 241$}{figure.caption.14}{}}
\citation{he2016deep}
\citation{ronneberger2015u}
\citation{wang2020towards}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces  Relative errors on 1-d Burgers' equation for different resolutions $s$.}}{57}{table.caption.13}\protected@file@percent }
\newlabel{table:burgers}{{3}{57}{Relative errors on 1-d Burgers' equation for different resolutions $s$}{table.caption.13}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.2.3}Zero-shot super-resolution.}{57}{subsubsection.7.2.3}\protected@file@percent }
\newlabel{sec:superresolution1}{{7.2.3}{57}{图神经网络（Graph Neural Networks）}{subsubsection.7.2.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.3}Navier-Stokes Equation}{57}{subsection.7.3}\protected@file@percent }
\newlabel{ssec:result_nse}{{7.3}{57}{图神经网络（Graph Neural Networks）}{subsection.7.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Benchmark on the Navier-Stokes}}{58}{figure.caption.15}\protected@file@percent }
\newlabel{fig:ns-error}{{10}{58}{Benchmark on the Navier-Stokes}{figure.caption.15}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces Benchmarks on Navier Stokes (fixing resolution $64 \times 64$ for both training and testing).}}{59}{table.caption.16}\protected@file@percent }
\newlabel{table:ns}{{4}{59}{Benchmarks on Navier Stokes (fixing resolution $64 \times 64$ for both training and testing)}{table.caption.16}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces  Resolution study on Navier-stokes equation ($\nu =10^{-3}$, $N=200$, $T=20$.)}}{59}{table.caption.17}\protected@file@percent }
\newlabel{table:nse}{{5}{59}{Resolution study on Navier-stokes equation ($\nu =10^{-3}$, $N=200$, $T=20$.)}{table.caption.17}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.3.1}Zero-shot super-resolution.}{60}{subsubsection.7.3.1}\protected@file@percent }
\newlabel{sec:superresolution}{{7.3.1}{60}{图神经网络（Graph Neural Networks）}{subsubsection.7.3.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces  Zero-shot super-resolution}}{60}{figure.caption.18}\protected@file@percent }
\newlabel{fig:super2}{{11}{60}{Zero-shot super-resolution}{figure.caption.18}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.3.2}Spectral analysis}{60}{subsubsection.7.3.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces The spectral decay of the predictions of different methods}}{61}{figure.caption.19}\protected@file@percent }
\newlabel{fig:spectra}{{12}{61}{The spectral decay of the predictions of different methods}{figure.caption.19}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces Spectral Decay in term of $k_{max}$ }}{61}{figure.caption.20}\protected@file@percent }
\newlabel{fig:spectral2}{{13}{61}{Spectral Decay in term of $k_{max}$}{figure.caption.20}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.3.3}Non-periodic boundary condition.}{61}{subsubsection.7.3.3}\protected@file@percent }
\citation{Cotter_2013}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.3.4}Bayesian Inverse Problem}{62}{subsubsection.7.3.4}\protected@file@percent }
\newlabel{sec:bayesian}{{7.3.4}{62}{图神经网络（Graph Neural Networks）}{subsubsection.7.3.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.4}Discussion and Comparison of the Four methods}{62}{subsection.7.4}\protected@file@percent }
\newlabel{ssec:comparsion}{{7.4}{62}{图神经网络（Graph Neural Networks）}{subsection.7.4}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.4.1}Ingenuity}{62}{subsubsection.7.4.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces Results of the Bayesian inverse problem for the Navier-Stokes equation. }}{63}{figure.caption.21}\protected@file@percent }
\newlabel{fig:baysian}{{14}{63}{Results of the Bayesian inverse problem for the Navier-Stokes equation}{figure.caption.21}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.4.2}Expressiveness}{63}{subsubsection.7.4.2}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {6}{\ignorespaces Ingenuity.}}{64}{table.caption.22}\protected@file@percent }
\newlabel{table:ingenuity}{{6}{64}{Ingenuity}{table.caption.22}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.4.3}Complexity}{64}{subsubsection.7.4.3}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {7}{\ignorespaces Complexity (roundup to second on a single Nvidia V100 GPU)}}{64}{table.caption.23}\protected@file@percent }
\newlabel{table:complexity}{{7}{64}{Complexity (roundup to second on a single Nvidia V100 GPU)}{table.caption.23}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.4.4}Refinability}{64}{subsubsection.7.4.4}\protected@file@percent }
\citation{lu2021comprehensive}
\citation{lu2021comprehensive}
\citation{pfaff2020learning}
\@writefile{lot}{\contentsline {table}{\numberline {8}{\ignorespaces Refinability.}}{65}{table.caption.24}\protected@file@percent }
\newlabel{table:refinability}{{8}{65}{Refinability}{table.caption.24}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.4.5}Robustness}{65}{subsubsection.7.4.5}\protected@file@percent }
\citation{wen2021u}
\citation{chen1995universal}
\citation{Kovachki}
\citation{chen1995universal}
\citation{lanthaler2021error}
\citation{lu2019deeponet,lu2021learning}
\citation{chen1995universal}
\citation{lanthaler2021error}
\citation{kovachki2021universal}
\@writefile{lot}{\contentsline {table}{\numberline {9}{\ignorespaces Robustness.}}{66}{table.caption.25}\protected@file@percent }
\newlabel{table:robustness}{{9}{66}{Robustness}{table.caption.25}{}}
\citation{Kovachki,kovachki2021universal}
\citation{chen1995universal,lanthaler2021error}
\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces Robustness on Advection and Burgers equations}}{67}{figure.caption.26}\protected@file@percent }
\newlabel{fig:robustness}{{15}{67}{Robustness on Advection and Burgers equations}{figure.caption.26}{}}
\@writefile{toc}{\contentsline {section}{\numberline {8}Approximation Theory}{67}{section.8}\protected@file@percent }
\newlabel{sec:approximation}{{8}{67}{图神经网络（Graph Neural Networks）}{section.8}{}}
\citation{grothendieck1955produits}
\citation{pinkus1999approximation}
\citation{lanthaler2021error,Kovachki}
\citation{lanthaler2021error}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.1}Neural Operators}{69}{subsection.8.1}\protected@file@percent }
\newlabel{sec:approximation_nos}{{8.1}{69}{图神经网络（Graph Neural Networks）}{subsection.8.1}{}}
\citation{lanthaler2021error}
\newlabel{def:bounded_activations}{{7}{70}{图神经网络（Graph Neural Networks）}{theorem.7}{}}
\citation{dudley2010concrete}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.2}Discretization Invariance}{71}{subsection.8.2}\protected@file@percent }
\newlabel{sec:discritizational_invariance}{{8.2}{71}{图神经网络（Graph Neural Networks）}{subsection.8.2}{}}
\newlabel{thm:discretizational_invariance}{{8}{71}{图神经网络（Graph Neural Networks）}{theorem.8}{}}
\citation{chen1995universal}
\citation{Kovachki}
\@writefile{lof}{\contentsline {figure}{\numberline {16}{\ignorespaces A schematic overview of the maps used to approximate \(\mathcal  {G}^\dagger \).}}{72}{figure.caption.27}\protected@file@percent }
\newlabel{fig:approach}{{16}{72}{A schematic overview of the maps used to approximate \(\G ^\dagger \)}{figure.caption.27}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.3}Approximation Theorems}{72}{subsection.8.3}\protected@file@percent }
\newlabel{sec:approximation_main}{{8.3}{72}{图神经网络（Graph Neural Networks）}{subsection.8.3}{}}
\newlabel{assump:input}{{9}{72}{图神经网络（Graph Neural Networks）}{theorem.9}{}}
\newlabel{assump:output}{{10}{72}{图神经网络（Graph Neural Networks）}{theorem.10}{}}
\newlabel{thm:main_compact}{{11}{73}{图神经网络（Graph Neural Networks）}{theorem.11}{}}
\citation{lanthaler2021error}
\citation{Kovachki}
\newlabel{thm:cm_compact}{{12}{74}{图神经网络（Graph Neural Networks）}{theorem.12}{}}
\newlabel{thm:measurable_approx}{{13}{74}{图神经网络（Graph Neural Networks）}{theorem.13}{}}
\citation{guo2016convolutional,Zabaras,Adler2017,bhatnagar2019prediction,kutyniok2022atheoretical}
\citation{khoo2017solving}
\citation{ummenhofer2020lagrangian}
\citation{jiang2020meshfreeflownet}
\citation{lu2019deeponet,lu2021learning}
\citation{chen1995universal}
\citation{chen1995universal}
\citation{lanthaler2021error}
\newlabel{thm:cm_measurable_approx}{{14}{75}{图神经网络（Graph Neural Networks）}{theorem.14}{}}
\@writefile{toc}{\contentsline {section}{\numberline {9}Literature Review}{75}{section.9}\protected@file@percent }
\newlabel{ssec:related-work}{{9}{75}{图神经网络（Graph Neural Networks）}{section.9}{}}
\citation{Weinan,raissi2019physics,sirignano2018dgm,bar2019unsupervised,smith2020eikonet,pan2020physics,beck2021solving}
\citation{pathak2020using,um2020solver,greenfeld2019learning}
\citation{DeVoreReducedBasis}
\citation{cohendevore}
\citation{Kovachki,nelsen2020random,opschoor2020deep,schwab2019deep,o2020derivative,lu2019deeponet,fresca2022poddlrom}
\citation{guo2016convolutional,Zabaras,Adler2017,bhatnagar2019prediction}
\citation{Weinan,raissi2019physics,herrmann2020deep,bar2019unsupervised}
\citation{haber2017stable,weinan2017proposal}
\citation{Williams,Neal,BengioLeRoux,GlobersonLivni,Guss}
\citation{Neal,MathewsGP,Garriga-AlonsoGP}
\citation{damianou2013deep,aretha}
\citation{nystrom1930praktische}
\citation{kipf2016semi,hamilton2017inductive,gilmer2017neural,velivckovic2017graph,murphy2018janossy}
\citation{chen2019graph}
\citation{battaglia2018relational}
\citation{pmlr-v97-alet19a}
\citation{kulis2006learning,bach2013sharp,lan2017low,gardner2018product}
\citation{khoo2019switchnet}
\citation{lu2019deeponet}
\citation{greengard1997new}
\citation{borm2003hierarchical}
\citation{kondor2014multiresolution,borm2003hierarchical}
\citation{fan2019multiscale,fan2019multiscale2,he2019mgnet}
\citation{hornik1989multilayer}
\citation{rahimi2008uniform}
\citation{mathieu2013fast}
\citation{bengio2007scaling,mingo2004Fourier,sitzmann2020implicit}
\citation{fan2019bcr,fan2019multiscale,kashinath2020enforcing}
\citation{kovachki2021universal}
\citation{lanthaler2021error,kovachki2021universal}
\@writefile{toc}{\contentsline {section}{\numberline {10}Conclusions}{80}{section.10}\protected@file@percent }
\newlabel{sec:conclusion}{{10}{80}{图神经网络（Graph Neural Networks）}{section.10}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.1}Future Directions}{81}{subsection.10.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {10.1.1}New Applications}{81}{subsubsection.10.1.1}\protected@file@percent }
\citation{pathak2020using,um2020solverintheloop}
\citation{wang2021learning,li2021physics}
\citation{lu2019deeponet}
\citation{devore1998nonlinear}
\citation{poole2016exponential}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {10.1.2}New Methodologies}{82}{subsubsection.10.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {10.1.3}Theory}{82}{subsubsection.10.1.3}\protected@file@percent }
\bibdata{ref}
\bibcite{aaronson1997introduction}{{1}{1997}{{Aaronson}}{{}}}
\bibcite{adams2003sobolev}{{2}{2003}{{Adams and Fournier}}{{}}}
\bibcite{Adler2017}{{3}{2017}{{Adler and Oktem}}{{}}}
\bibcite{albiac2006topics}{{4}{2006}{{Albiac and Kalton}}{{}}}
\bibcite{pmlr-v97-alet19a}{{5}{2019}{{Alet et~al.}}{{Alet, Jeewajee, Villalonga, Rodriguez, Lozano-Perez, and Kaelbling}}}
\bibcite{ba2016layer}{{6}{2016}{{Ba et~al.}}{{Ba, Kiros, and Hinton}}}
\bibcite{bach2013sharp}{{7}{2013}{{Bach}}{{}}}
\bibcite{bar2019unsupervised}{{8}{2019}{{Bar and Sochen}}{{}}}
\bibcite{battaglia2018relational}{{9}{2018}{{Battaglia et~al.}}{{Battaglia, Hamrick, Bapst, Sanchez-Gonzalez, Zambaldi, Malinowski, Tacchetti, Raposo, Santoro, Faulkner, et~al.}}}
\bibcite{beck2021solving}{{10}{2021}{{Beck et~al.}}{{Beck, Becker, Grohs, Jaafari, and Jentzen}}}
\bibcite{belongie2002spectral}{{11}{2002}{{Belongie et~al.}}{{Belongie, Fowlkes, Chung, and Malik}}}
\bibcite{bengio2007scaling}{{12}{2007}{{Bengio et~al.}}{{Bengio, LeCun, et~al.}}}
\bibcite{bhatnagar2019prediction}{{13}{2019}{{Bhatnagar et~al.}}{{Bhatnagar, Afshar, Pan, Duraisamy, and Kaushik}}}
\bibcite{Kovachki}{{14}{2020}{{Bhattacharya et~al.}}{{Bhattacharya, Hosseini, Kovachki, and Stuart}}}
\bibcite{bogachev2007measure}{{15}{2007}{{Bogachev}}{{}}}
\bibcite{bonito2020nonlinear}{{16}{2020}{{Bonito et~al.}}{{Bonito, Cohen, DeVore, Guignard, Jantsch, and Petrova}}}
\bibcite{borm2003hierarchical}{{17}{2003}{{B{\"o}rm et~al.}}{{B{\"o}rm, Grasedyck, and Hackbusch}}}
\bibcite{box1976science}{{18}{1976}{{Box}}{{}}}
\bibcite{boyd2001chebyshev}{{19}{2001}{{Boyd}}{{}}}
\bibcite{brown2020language}{{20}{2020}{{Brown et~al.}}{{Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal, Neelakantan, Shyam, Sastry, Askell, et~al.}}}
\bibcite{brudnyi2012methods}{{21}{2012}{{Brudnyi and Brudnyi}}{{}}}
\bibcite{bruno2007accurate}{{22}{2007}{{Bruno et~al.}}{{Bruno, Han, and Pohlman}}}
\bibcite{chandler2013invariant}{{23}{2013}{{Chandler and Kerswell}}{{}}}
\bibcite{chen2019graph}{{24}{2019}{{Chen et~al.}}{{Chen, Ye, Zuo, Zheng, and Ong}}}
\bibcite{chen1995universal}{{25}{1995}{{Chen and Chen}}{{}}}
\bibcite{choromanski2020rethinking}{{26}{2020}{{Choromanski et~al.}}{{Choromanski, Likhosherstov, Dohan, Song, Gane, Sarlos, Hawkins, Davis, Mohiuddin, Kaiser, et~al.}}}
\bibcite{ciesielski1972construction}{{27}{1972}{{Ciesielski and Domsta}}{{}}}
\bibcite{cohendevore}{{28}{2015}{{Cohen and DeVore}}{{}}}
\bibcite{cohen2020optimal}{{29}{2020}{{Cohen et~al.}}{{Cohen, Devore, Petrova, and Wojtaszczyk}}}
\bibcite{conway2007acourse}{{30}{2007}{{Conway}}{{}}}
\bibcite{Cotter_2013}{{31}{2013}{{Cotter et~al.}}{{Cotter, Roberts, Stuart, and White}}}
\bibcite{cotter2009bayesian}{{32}{2009}{{Cotter et~al.}}{{Cotter, Dashti, Robinson, and Stuart}}}
\bibcite{damianou2013deep}{{33}{2013}{{Damianou and Lawrence}}{{}}}
\bibcite{de2022cost}{{34}{2022}{{De~Hoop et~al.}}{{De~Hoop, Huang, Qian, and Stuart}}}
\bibcite{devlin2018bert}{{35}{2018}{{Devlin et~al.}}{{Devlin, Chang, Lee, and Toutanova}}}
\bibcite{devore1998nonlinear}{{36}{1998}{{DeVore}}{{}}}
\bibcite{DeVoreReducedBasis}{{37}{2014}{{DeVore}}{{}}}
\bibcite{dosovitskiy2020image}{{38}{2020}{{Dosovitskiy et~al.}}{{Dosovitskiy, Beyer, Kolesnikov, Weissenborn, Zhai, Unterthiner, Dehghani, Minderer, Heigold, Gelly, et~al.}}}
\bibcite{dudley2011concrete}{{39}{2011}{{Dudley and Norvaisa}}{{}}}
\bibcite{dudley2010concrete}{{40}{2010}{{Dudley and Norvai{\v {s}}a}}{{}}}
\bibcite{dugundji1961anextension}{{41}{1951}{{Dugundji}}{{}}}
\bibcite{aretha}{{42}{2018}{{Dunlop et~al.}}{{Dunlop, Girolami, Stuart, and Teckentrup}}}
\bibcite{weinan2017proposal}{{43}{2017}{{E}}{{}}}
\bibcite{e2011principles}{{44}{2011}{{E}}{{}}}
\bibcite{Weinan}{{45}{2018}{{E and Yu}}{{}}}
\bibcite{fan2019bcr}{{46}{2019{a}}{{Fan et~al.}}{{Fan, Bohorquez, and Ying}}}
\bibcite{fan2019multiscale2}{{47}{2019{b}}{{Fan et~al.}}{{Fan, Feliu-Faba, Lin, Ying, and Zepeda-N{\'u}nez}}}
\bibcite{fan2019multiscale}{{48}{2019{c}}{{Fan et~al.}}{{Fan, Lin, Ying, and Zepeda-N{\'u}nez}}}
\bibcite{fefferman2007cmextension}{{49}{2007}{{Fefferman}}{{}}}
\bibcite{fresca2022poddlrom}{{50}{2022}{{Fresca and Manzoni}}{{}}}
\bibcite{gardner2018product}{{51}{2018}{{Gardner et~al.}}{{Gardner, Pleiss, Wu, Weinberger, and Wilson}}}
\bibcite{Garriga-AlonsoGP}{{52}{2018}{{{Garriga-Alonso} et~al.}}{{{Garriga-Alonso}, {Rasmussen}, and {Aitchison}}}}
\bibcite{gilmer2017neural}{{53}{2017}{{Gilmer et~al.}}{{Gilmer, Schoenholz, Riley, Vinyals, and Dahl}}}
\bibcite{GlobersonLivni}{{54}{2016}{{Globerson and Livni}}{{}}}
\bibcite{greenfeld2019learning}{{55}{2019}{{Greenfeld et~al.}}{{Greenfeld, Galun, Basri, Yavneh, and Kimmel}}}
\bibcite{greengard1997new}{{56}{1997}{{Greengard and Rokhlin}}{{}}}
\bibcite{grothendieck1955produits}{{57}{1955}{{Grothendieck}}{{}}}
\bibcite{guibas2021adaptive}{{58}{2021}{{Guibas et~al.}}{{Guibas, Mardani, Li, Tao, Anandkumar, and Catanzaro}}}
\bibcite{guo2016convolutional}{{59}{2016}{{Guo et~al.}}{{Guo, Li, and Iorio}}}
\bibcite{gurtin1982introduction}{{60}{1982}{{Gurtin}}{{}}}
\bibcite{Guss}{{61}{2016}{{Guss}}{{}}}
\bibcite{haber2017stable}{{62}{2017}{{Haber and Ruthotto}}{{}}}
\bibcite{hamilton2017inductive}{{63}{2017}{{Hamilton et~al.}}{{Hamilton, Ying, and Leskovec}}}
\bibcite{he2019mgnet}{{64}{2019}{{He and Xu}}{{}}}
\bibcite{he2016deep}{{65}{2016}{{He et~al.}}{{He, Zhang, Ren, and Sun}}}
\bibcite{herrmann2020deep}{{66}{2020}{{Herrmann et~al.}}{{Herrmann, Schwab, and Zech}}}
\bibcite{hornik1989multilayer}{{67}{1989}{{Hornik et~al.}}{{Hornik, Stinchcombe, White, et~al.}}}
\bibcite{jiang2020meshfreeflownet}{{68}{2020}{{Jiang et~al.}}{{Jiang, Esmaeilzadeh, Azizzadenesheli, Kashinath, Mustafa, Tchelepi, Marcus, Anandkumar, et~al.}}}
\bibcite{johnson2012numerical}{{69}{2012}{{Johnson}}{{}}}
\bibcite{kashinath2020enforcing}{{70}{2020}{{Kashinath et~al.}}{{Kashinath, Marcus, et~al.}}}
\bibcite{khoo2019switchnet}{{71}{2019}{{Khoo and Ying}}{{}}}
\bibcite{khoo2017solving}{{72}{2021}{{Khoo et~al.}}{{Khoo, Lu, and Ying}}}
\bibcite{kipf2016semi}{{73}{2016}{{Kipf and Welling}}{{}}}
\bibcite{kondor2014multiresolution}{{74}{2014}{{Kondor et~al.}}{{Kondor, Teneva, and Garg}}}
\bibcite{kovachki2021universal}{{75}{2021}{{Kovachki et~al.}}{{Kovachki, Lanthaler, and Mishra}}}
\bibcite{kraichnan67inertial}{{76}{1967}{{Kraichnan}}{{}}}
\bibcite{kulis2006learning}{{77}{2006}{{Kulis et~al.}}{{Kulis, Sustik, and Dhillon}}}
\bibcite{kutyniok2022atheoretical}{{78}{2022}{{Kutyniok et~al.}}{{Kutyniok, Petersen, Raslan, and Schneider}}}
\bibcite{lan2017low}{{79}{2017}{{Lan et~al.}}{{Lan, Zhang, Ge, Cheng, Liu, Rauber, Li, Wang, and Zha}}}
\bibcite{lanthaler2021error}{{80}{2021}{{Lanthaler et~al.}}{{Lanthaler, Mishra, and Karniadakis}}}
\bibcite{leoni2009first}{{81}{2009}{{Leoni}}{{}}}
\bibcite{li2020fourier}{{82}{2020{a}}{{Li et~al.}}{{Li, Kovachki, Azizzadenesheli, Liu, Bhattacharya, Stuart, and Anandkumar}}}
\bibcite{li2020multipole}{{83}{2020{b}}{{Li et~al.}}{{Li, Kovachki, Azizzadenesheli, Liu, Bhattacharya, Stuart, and Anandkumar}}}
\bibcite{li2020neural}{{84}{2020{c}}{{Li et~al.}}{{Li, Kovachki, Azizzadenesheli, Liu, Bhattacharya, Stuart, and Anandkumar}}}
\bibcite{li2021physics}{{85}{2021}{{Li et~al.}}{{Li, Zheng, Kovachki, Jin, Chen, Liu, Azizzadenesheli, and Anandkumar}}}
\bibcite{lu2019deeponet}{{86}{2019}{{Lu et~al.}}{{Lu, Jin, and Karniadakis}}}
\bibcite{lu2021learning}{{87}{2021{a}}{{Lu et~al.}}{{Lu, Jin, Pang, Zhang, and Karniadakis}}}
\bibcite{lu2021comprehensive}{{88}{2021{b}}{{Lu et~al.}}{{Lu, Meng, Cai, Mao, Goswami, Zhang, and Karniadakis}}}
\bibcite{mathieu2013fast}{{89}{2013}{{Mathieu et~al.}}{{Mathieu, Henaff, and LeCun}}}
\bibcite{MathewsGP}{{90}{2018}{{{Matthews} et~al.}}{{{Matthews}, {Rowland}, {Hron}, {Turner}, and {Ghahramani}}}}
\bibcite{mingo2004Fourier}{{91}{2004}{{Mingo et~al.}}{{Mingo, Aslanyan, Castellanos, Diaz, and Riazanov}}}
\bibcite{murphy2018janossy}{{92}{2018}{{Murphy et~al.}}{{Murphy, Srinivasan, Rao, and Ribeiro}}}
\bibcite{Neal}{{93}{1996}{{Neal}}{{}}}
\bibcite{nelsen2020random}{{94}{2021}{{Nelsen and Stuart}}{{}}}
\bibcite{nystrom1930praktische}{{95}{1930}{{Nystr{\"o}m}}{{}}}
\bibcite{o2020derivative}{{96}{2020}{{O'Leary-Roseberry et~al.}}{{O'Leary-Roseberry, Villa, Chen, and Ghattas}}}
\bibcite{opschoor2020deep}{{97}{2020}{{Opschoor et~al.}}{{Opschoor, Schwab, and Zech}}}
\bibcite{pan2020physics}{{98}{2020}{{Pan and Duraisamy}}{{}}}
\bibcite{pathak2020using}{{99}{2020}{{Pathak et~al.}}{{Pathak, Mustafa, Kashinath, Motheau, Kurth, and Day}}}
\bibcite{pelczynski2001contribution}{{100}{2001}{{Pełczyński and Wojciechowski}}{{}}}
\bibcite{pfaff2020learning}{{101}{2020}{{Pfaff et~al.}}{{Pfaff, Fortunato, Sanchez-Gonzalez, and Battaglia}}}
\bibcite{pinkus1985nwidths}{{102}{1985}{{Pinkus}}{{}}}
\bibcite{pinkus1999approximation}{{103}{1999}{{Pinkus}}{{}}}
\bibcite{poole2016exponential}{{104}{2016}{{Poole et~al.}}{{Poole, Lahiri, Raghu, Sohl-Dickstein, and Ganguli}}}
\bibcite{quinonero2005aunifying}{{105}{2005}{{Qui\~{n}onero Candela and Rasmussen}}{{}}}
\bibcite{rahimi2008uniform}{{106}{2008}{{Rahimi and Recht}}{{}}}
\bibcite{raissi2019physics}{{107}{2019}{{Raissi et~al.}}{{Raissi, Perdikaris, and Karniadakis}}}
\bibcite{ronneberger2015u}{{108}{2015}{{Ronneberger et~al.}}{{Ronneberger, Fischer, and Brox}}}
\bibcite{BengioLeRoux}{{109}{2007}{{Roux and Bengio}}{{}}}
\bibcite{scarselli2008graph}{{110}{2008}{{Scarselli et~al.}}{{Scarselli, Gori, Tsoi, Hagenbuchner, and Monfardini}}}
\bibcite{schwab2019deep}{{111}{2019}{{Schwab and Zech}}{{}}}
\bibcite{sirignano2018dgm}{{112}{2018}{{Sirignano and Spiliopoulos}}{{}}}
\bibcite{sitzmann2020implicit}{{113}{2020}{{Sitzmann et~al.}}{{Sitzmann, Martel, Bergman, Lindell, and Wetzstein}}}
\bibcite{smith2020eikonet}{{114}{2020}{{Smith et~al.}}{{Smith, Azizzadenesheli, and Ross}}}
\bibcite{stein1970singular}{{115}{1970}{{Stein}}{{}}}
\bibcite{stuart_2010}{{116}{2010}{{Stuart}}{{}}}
\bibcite{trefethen2000spectral}{{117}{2000}{{Trefethen}}{{}}}
\bibcite{trillos2018variational}{{118}{2018}{{Trillos and Slep{\v {c}}ev}}{{}}}
\bibcite{trillos2020error}{{119}{2020}{{Trillos et~al.}}{{Trillos, Gerlach, Hein, and Slep{\v {c}}ev}}}
\bibcite{um2020solver}{{120}{2020{a}}{{Um et~al.}}{{Um, Holl, Brand, Thuerey, et~al.}}}
\bibcite{um2020solverintheloop}{{121}{2020{b}}{{Um et~al.}}{{Um, Raymond, Fei, Holl, Brand, and Thuerey}}}
\bibcite{ummenhofer2020lagrangian}{{122}{2020}{{Ummenhofer et~al.}}{{Ummenhofer, Prantl, Th{\"u}rey, and Koltun}}}
\bibcite{Vapnik1998}{{123}{1998}{{Vapnik}}{{}}}
\bibcite{vaswani2017attention}{{124}{2017}{{Vaswani et~al.}}{{Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez, Kaiser, and Polosukhin}}}
\bibcite{velivckovic2017graph}{{125}{2017}{{Veli{\v {c}}kovi{\'c} et~al.}}{{Veli{\v {c}}kovi{\'c}, Cucurull, Casanova, Romero, Lio, and Bengio}}}
\bibcite{von2008consistency}{{126}{2008}{{Von~Luxburg et~al.}}{{Von~Luxburg, Belkin, and Bousquet}}}
\bibcite{wang2020towards}{{127}{2020}{{Wang et~al.}}{{Wang, Kashinath, Mustafa, Albert, and Yu}}}
\bibcite{wang2021learning}{{128}{2021}{{Wang et~al.}}{{Wang, Wang, and Perdikaris}}}
\bibcite{wen2021u}{{129}{2021}{{Wen et~al.}}{{Wen, Li, Azizzadenesheli, Anandkumar, and Benson}}}
\bibcite{whitney1934functions}{{130}{1934}{{Whitney}}{{}}}
\bibcite{Williams}{{131}{1996}{{Williams}}{{}}}
\bibcite{zhu2021long}{{132}{2021}{{Zhu et~al.}}{{Zhu, Ping, Xiao, Shoeybi, Goldstein, Anandkumar, and Catanzaro}}}
\bibcite{Zabaras}{{133}{2018}{{Zhu and Zabaras}}{{}}}
\@writefile{toc}{\contentsline {section}{\numberline {A}}{98}{section.1}\protected@file@percent }
\newlabel{sec:appendix_notation}{{A}{98}{Acknowledgements}{section.1}{}}
\@writefile{lot}{\contentsline {table}{\numberline {10}{\ignorespaces Table of notations: operator learning and neural operators}}{98}{table.caption.29}\protected@file@percent }
\newlabel{table:notations1}{{10}{98}{Table of notations: operator learning and neural operators}{table.caption.29}{}}
\citation{whitney1934functions,brudnyi2012methods}
\citation{adams2003sobolev}
\@writefile{toc}{\contentsline {section}{\numberline {B}}{100}{section.2}\protected@file@percent }
\newlabel{sec:appendix_approximationproperty}{{B}{100}{Acknowledgements}{section.2}{}}
\newlabel{def:schauder_bases}{{15}{100}{Acknowledgements}{theorem.15}{}}
\citation{albiac2006topics}
\citation{albiac2006topics}
\newlabel{def:finiterank}{{16}{101}{Acknowledgements}{theorem.16}{}}
\newlabel{def:ap}{{17}{101}{Acknowledgements}{theorem.17}{}}
\newlabel{lemma:schauder_ap}{{18}{101}{Acknowledgements}{theorem.18}{}}
\newlabel{lemma:schauder_schauder}{{19}{102}{Acknowledgements}{theorem.19}{}}
\newlabel{lemma:ap_ap}{{20}{103}{Acknowledgements}{theorem.20}{}}
\newlabel{lemma:compact_union}{{21}{103}{Acknowledgements}{theorem.21}{}}
\citation{Kovachki}
\newlabel{lemma:finitedim_approx}{{22}{105}{Acknowledgements}{theorem.22}{}}
\newlabel{lemma:cm_isomorphism}{{23}{107}{Acknowledgements}{theorem.23}{}}
\newlabel{corr:cm_isomorphism}{{24}{107}{Acknowledgements}{theorem.24}{}}
\newlabel{eq:tau}{{50}{108}{Acknowledgements}{equation.50}{}}
\newlabel{lemma:w1_isomorphism}{{25}{108}{Acknowledgements}{theorem.25}{}}
\citation{albiac2006topics}
\citation{stein1970singular}
\citation{pelczynski2001contribution}
\citation{albiac2006topics}
\citation{ciesielski1972construction}
\citation{fefferman2007cmextension}
\newlabel{lemma:ap}{{26}{109}{Acknowledgements}{theorem.26}{}}
\citation{ciesielski1972construction}
\citation{stein1970singular}
\citation{pelczynski2001contribution}
\citation{leoni2009first}
\@writefile{toc}{\contentsline {section}{\numberline {C}}{111}{section.3}\protected@file@percent }
\newlabel{sec:appendix_functionalapprox}{{C}{111}{Acknowledgements}{section.3}{}}
\newlabel{lemma:reisz}{{27}{111}{Acknowledgements}{theorem.27}{}}
\citation{leoni2009first}
\citation{conway2007acourse}
\citation{adams2003sobolev}
\newlabel{lemma:wmp_kernelapprox}{{28}{112}{Acknowledgements}{theorem.28}{}}
\citation{leoni2009first}
\citation{adams2003sobolev}
\citation{bogachev2007measure}
\newlabel{lemma:cm_delta_approx}{{29}{116}{Acknowledgements}{theorem.29}{}}
\newlabel{lemma:c_kernelapprox}{{30}{118}{Acknowledgements}{theorem.30}{}}
\newlabel{lemma:cm_kernelapprox}{{31}{119}{Acknowledgements}{theorem.31}{}}
\@writefile{toc}{\contentsline {section}{\numberline {D}}{120}{section.4}\protected@file@percent }
\newlabel{sec:appendix_nos}{{D}{120}{Acknowledgements}{section.4}{}}
\newlabel{lemma:input_approx}{{32}{120}{Acknowledgements}{theorem.32}{}}
\newlabel{lemma:cm_input_approx}{{33}{121}{Acknowledgements}{theorem.33}{}}
\citation{leoni2009first}
\newlabel{lemma:output_approx}{{34}{122}{Acknowledgements}{theorem.34}{}}
\newlabel{lemma:nn_emulation}{{35}{124}{Acknowledgements}{theorem.35}{}}
\@writefile{toc}{\contentsline {section}{\numberline {E}}{124}{section.5}\protected@file@percent }
\newlabel{sec_proof:discretizational_invariance}{{E}{124}{Acknowledgements}{section.5}{}}
\newlabel{eq:disc_inv_modulusofcont}{{51}{125}{Acknowledgements}{equation.51}{}}
\newlabel{eq:disc_inv_needtoshow}{{52}{125}{Acknowledgements}{equation.52}{}}
\citation{pinkus1999approximation}
\@writefile{toc}{\contentsline {section}{\numberline {F}}{127}{section.6}\protected@file@percent }
\newlabel{sec_proof:main_compact}{{F}{127}{Acknowledgements}{section.6}{}}
\citation{aaronson1997introduction}
\citation{dugundji1961anextension}
\@writefile{toc}{\contentsline {section}{\numberline {G}}{130}{section.7}\protected@file@percent }
\newlabel{sec_proof:measurable_approx}{{G}{130}{Acknowledgements}{section.7}{}}
\newlabel{LastPage}{{G}{131}{Acknowledgements}{page.131}{}}
\gdef\lastpage@lastpage{131}
\gdef\lastpage@lastpageHy{131}
\gdef \@abspage@last{131}
